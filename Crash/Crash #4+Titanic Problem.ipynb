{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       PassengerId    Survived       Pclass          Age        SibSp        Parch         Fare\n",
      "count  1309.000000  891.000000  1309.000000  1046.000000  1309.000000  1309.000000  1308.000000\n",
      "mean    655.000000    0.383838     2.294882    29.881138     0.498854     0.385027    33.295479\n",
      "std     378.020061    0.486592     0.837836    14.413493     1.041658     0.865560    51.758668\n",
      "min       1.000000    0.000000     1.000000     0.170000     0.000000     0.000000     0.000000\n",
      "25%     328.000000    0.000000     2.000000    21.000000     0.000000     0.000000     7.895800\n",
      "50%     655.000000    0.000000     3.000000    28.000000     0.000000     0.000000    14.454200\n",
      "75%     982.000000    1.000000     3.000000    39.000000     1.000000     0.000000    31.275000\n",
      "max    1309.000000    1.000000     3.000000    80.000000     8.000000     9.000000   512.329200\n",
      "   Pclass   Age     Fare  Survived  Sex_male  Embarked_Q  Embarked_S  Title_Miss  Title_Mr  Title_Mrs  Title_Officer  Title_Royalty  Family_Single  Family_Small  Cabin_B  Cabin_C  Cabin_D  Cabin_E  Cabin_F  Cabin_G  Cabin_T  Cabin_U\n",
      "0       3  22.0   7.2500       0.0         1           0           1           0         1          0              0              0              0             1        0        0        0        0        0        0        0        1\n",
      "1       1  38.0  71.2833       1.0         0           0           0           0         0          1              0              0              0             1        0        1        0        0        0        0        0        0\n",
      "2       3  26.0   7.9250       1.0         0           0           1           1         0          0              0              0              1             0        0        0        0        0        0        0        0        1\n",
      "3       1  35.0  53.1000       1.0         0           0           1           0         0          1              0              0              0             1        0        1        0        0        0        0        0        0\n",
      "4       3  35.0   8.0500       0.0         1           0           1           0         1          0              0              0              1             0        0        0        0        0        0        0        0        1\n",
      "\n",
      "Metrics for Regression:\n",
      "    Accuracy: 0.70 (+/- 0.08)\n",
      "    F1: 0.44 (+/- 0.13)\n",
      "    Precision: 0.75 (+/- 0.24)\n",
      "    ROC AUC: 0.78 (+/- 0.11)\n",
      "\n",
      "Metrics for Random forest:\n",
      "    Accuracy: 0.78 (+/- 0.04)\n",
      "    F1: 0.70 (+/- 0.07)\n",
      "    Precision: 0.73 (+/- 0.03)\n",
      "    ROC AUC: 0.86 (+/- 0.02)\n",
      "\n",
      "Metrics for Adaboost:\n",
      "    Accuracy: 0.81 (+/- 0.05)\n",
      "    F1: 0.75 (+/- 0.06)\n",
      "    Precision: 0.76 (+/- 0.10)\n",
      "    ROC AUC: 0.86 (+/- 0.06)\n",
      "\n",
      "Metrics for Gradient boosting:\n",
      "    Accuracy: 0.81 (+/- 0.08)\n",
      "    F1: 0.72 (+/- 0.06)\n",
      "    Precision: 0.77 (+/- 0.07)\n",
      "    ROC AUC: 0.84 (+/- 0.06)\n",
      "\n",
      "Metrics for SVM:\n",
      "    Accuracy: 0.73 (+/- 0.07)\n",
      "    F1: 0.63 (+/- 0.07)\n",
      "    Precision: 0.66 (+/- 0.12)\n",
      "    ROC AUC: 0.78 (+/- 0.09)\n"
     ]
    }
   ],
   "source": [
    "# ===========================================================================\n",
    "# ============ LIBRARIES IMPORT, CONSTANT and HELPER FUNCTION ===============\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import warnings\n",
    "\n",
    "\n",
    "# To get rid of the annoying ConvergenceWarning\n",
    "warnings.filterwarnings('ignore', 'The max_iter.*')\n",
    "\n",
    "# Help to print wilder, and see more of a dataframe on screen\n",
    "DESIRED_WIDTH = 500\n",
    "pd.set_option(\"display.width\", DESIRED_WIDTH)\n",
    "\n",
    "\n",
    "# Little helper function, because we re-use it; it prints wilder, so that we see more columns\n",
    "def print_wilder(to_be_printed):\n",
    "    with pd.option_context(\"display.max_rows\", None, \"display.max_columns\", 205):\n",
    "        print(to_be_printed)\n",
    "\n",
    "\n",
    "# Helper function for the re-using of the function to get the different scoring associated with the cross-validation\n",
    "def cross_validate_and_metrics(name_of_model, trained_model):\n",
    "    scores = cross_val_score(trained_model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    scores2 = cross_val_score(trained_model, X_train, y_train, cv=5, scoring='f1')\n",
    "    scores3 = cross_val_score(trained_model, X_train, y_train, cv=5, scoring='precision')\n",
    "    scores4 = cross_val_score(trained_model, X_train, y_train, cv=5, scoring='roc_auc')\n",
    "    print()\n",
    "    # print(scores)\n",
    "    print(f\"Metrics for {name_of_model}:\")\n",
    "    print(f\"    Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "    print(f\"    F1: %0.2f (+/- %0.2f)\" % (scores2.mean(), scores2.std() * 2))\n",
    "    print(f\"    Precision: %0.2f (+/- %0.2f)\" % (scores3.mean(), scores3.std() * 2))\n",
    "    print(f\"    ROC AUC: %0.2f (+/- %0.2f)\" % (scores4.mean(), scores4.std() * 2))\n",
    "\n",
    "\n",
    "# ===========================================================================\n",
    "# ================= IMPORTING DATA, OVERVIEW, CHOOSING FEATURES  ============\n",
    "\n",
    "# load data\n",
    "train = pd.read_csv(\"trainTitanic.csv\")\n",
    "test = pd.read_csv(\"testTitanic.csv\")\n",
    "# save PassengerId for final submission\n",
    "passengerId = test.PassengerId\n",
    "\n",
    "# merge train and test, to avoid doing things twice\n",
    "df = train.append(test, ignore_index=True, sort=False)\n",
    "# create indexes to separate data later on\n",
    "train_idx = len(train) - 1\n",
    "test_idx = len(df) - len(test)\n",
    "\n",
    "print_wilder(df.describe())  # Among other things, good to find features with NaN\n",
    "\n",
    "# Choosing baseline features\n",
    "features_kept = [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"Survived\", \"Name\", \"Cabin\", \"Parch\", \"SibSp\"]\n",
    "all_data_baseline = df.copy()\n",
    "data_baseline = all_data_baseline.loc[:, features_kept]\n",
    "\n",
    "\n",
    "# ===========================================================================\n",
    "# ============== FEATURES ENGINEERING, IMPUTING MISSING DATA ================\n",
    "\n",
    "# create a new feature to extract title names from the Name column\n",
    "data_baseline[\"Title\"] = data_baseline[\"Name\"].apply(lambda name: name.split(\",\")[1].split(\".\")[0].strip())\n",
    "\n",
    "normalized_titles = {\n",
    "    \"Capt\":       \"Officer\",\n",
    "    \"Col\":        \"Officer\",\n",
    "    \"Major\":      \"Officer\",\n",
    "    \"Jonkheer\":   \"Royalty\",\n",
    "    \"Don\":        \"Royalty\",\n",
    "    \"Sir\":       \"Royalty\",\n",
    "    \"Dr\":         \"Officer\",\n",
    "    \"Rev\":        \"Officer\",\n",
    "    \"the Countess\": \"Royalty\",\n",
    "    \"Dona\":       \"Royalty\",\n",
    "    \"Mme\":        \"Mrs\",\n",
    "    \"Mlle\":       \"Miss\",\n",
    "    \"Ms\":         \"Mrs\",\n",
    "    \"Mr\":        \"Mr\",\n",
    "    \"Mrs\":       \"Mrs\",\n",
    "    \"Miss\":      \"Miss\",\n",
    "    \"Master\":    \"Master\",\n",
    "    \"Lady\":      \"Royalty\"\n",
    "}\n",
    "\n",
    "# map the normalized titles to the current titles\n",
    "data_baseline[\"Title\"] = data_baseline[\"Title\"].map(normalized_titles)\n",
    "\n",
    "# group by Sex, Pclass, and Title\n",
    "subGroupFeatures = data_baseline.groupby([\"Sex\", \"Pclass\", \"Title\"])\n",
    "\n",
    "# apply the grouped median value on the Age NaN\n",
    "data_baseline[\"Age\"] = subGroupFeatures.Age.apply(lambda x: x.fillna(x.median()))\n",
    "\n",
    "# fill Cabin NaN with U for unknown\n",
    "data_baseline[\"Cabin\"] = data_baseline[\"Cabin\"].fillna('U')\n",
    "# find most frequent Embarked value and store it\n",
    "most_embarked = data_baseline[\"Embarked\"].value_counts().index[0]\n",
    "\n",
    "# fill NaN with most_embarked value\n",
    "data_baseline[\"Embarked\"] = data_baseline[\"Embarked\"].fillna(most_embarked)\n",
    "# fill NaN with median fare\n",
    "data_baseline[\"Fare\"] = data_baseline[\"Fare\"].fillna(data_baseline[\"Fare\"].median())\n",
    "\n",
    "# size of families (including the passenger)\n",
    "data_baseline[\"FamilySize\"] = data_baseline[\"Parch\"] + data_baseline[\"SibSp\"] + 1\n",
    "data_baseline = data_baseline.drop([\"Parch\", \"SibSp\"], axis=1)  # axis=1 because it's a column\n",
    "family_size_cat = {1: \"Single\", 2: \"Small\", 3: \"Small\", 4: \"Small\", 5: \"Large\", 6: \"Large\", 7: \"Large\", 8: \"Large\",\n",
    "                   11: \"Large\"}\n",
    "data_baseline[\"FamilySize\"] = data_baseline[\"FamilySize\"].map(family_size_cat)\n",
    "\n",
    "# map first letter of cabin to itself\n",
    "data_baseline.Cabin = data_baseline.Cabin.map(lambda x: x[0])  # first letter is the section\n",
    "\n",
    "\n",
    "# ===========================================================================\n",
    "# =================== DUMMIFYING CATEGORICAL VARIABLES ======================\n",
    "\n",
    "# dummify the categorical variables Sex, Embarked, Title, FamilySize, Cabin\n",
    "# drop_first nous laisse n-1 dummies\n",
    "dummies_sex = pd.get_dummies(data_baseline.loc[:, \"Sex\"], drop_first=True, prefix=\"Sex\")\n",
    "dummies_embarked = pd.get_dummies(data_baseline.loc[:, \"Embarked\"], drop_first=True, prefix=\"Embarked\")\n",
    "dummies_title = pd.get_dummies(data_baseline.loc[:, \"Title\"], drop_first=True, prefix=\"Title\")\n",
    "dummies_family = pd.get_dummies(data_baseline.loc[:, \"FamilySize\"], drop_first=True, prefix=\"Family\")\n",
    "dummies_cabin = pd.get_dummies(data_baseline.loc[:, \"Cabin\"], drop_first=True, prefix=\"Cabin\")\n",
    "data = pd.concat([data_baseline, dummies_sex, dummies_embarked, dummies_title, dummies_family, dummies_cabin], axis=1)\n",
    "data = data.drop([\"Sex\", \"Embarked\", \"Title\", \"Name\", \"Cabin\", \"FamilySize\"], axis=1)\n",
    "print_wilder(data.head())\n",
    "\n",
    "\n",
    "# ===========================================================================\n",
    "# ======================== RE SPLIT OF TRAIN/TEST ===========================\n",
    "\n",
    "# create train and test data\n",
    "df_train = data.loc[: train_idx]\n",
    "test = data[test_idx:]\n",
    "# convert Survived back to int\n",
    "train.loc[:, \"Survived\"] = train.loc[:, \"Survived\"].astype(int)  # Note for me, first instance of the warning for .loc\n",
    "\n",
    "# create X and y for data and target values\n",
    "X_train = df_train.drop(\"Survived\", axis=1).values\n",
    "y_train = df_train.loc[:, \"Survived\"].values\n",
    "# create array for test set\n",
    "X_test = test.drop(\"Survived\", axis=1).values\n",
    "\n",
    "\n",
    "# ===========================================================================\n",
    "# =============== MODEL 1: SIMPLE REGRESSION AS BASELINE ====================\n",
    "\n",
    "trained_model_regression = LogisticRegression(random_state=0, solver='sag')\n",
    "cross_validate_and_metrics(\"Regression\", trained_model_regression)\n",
    "\n",
    "\n",
    "# ===========================================================================\n",
    "# ====================== MODEL 2: RANDOM FOREST =============================\n",
    "\n",
    "trained_model_random_forest = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=0)\n",
    "cross_validate_and_metrics(\"Random forest\", trained_model_random_forest)\n",
    "\n",
    "\n",
    "# ===========================================================================\n",
    "# ======================== MODEL 3: ADABOOST ================================\n",
    "\n",
    "trained_model_adaboost = AdaBoostClassifier()\n",
    "cross_validate_and_metrics(\"Adaboost\", trained_model_adaboost)\n",
    "\n",
    "\n",
    "# ===========================================================================\n",
    "# ==================== MODEL 4: GRADIENT BOOSTING ===========================\n",
    "\n",
    "trained_model_gradient = GradientBoostingClassifier(criterion=\"friedman_mse\", learning_rate=0.2, loss=\"deviance\",\n",
    "                                                    max_depth=3, max_features=\"sqrt\", min_samples_leaf=0.1,\n",
    "                                                    min_samples_split=0.17273, n_estimators=10, subsample=1)\n",
    "cross_validate_and_metrics(\"Gradient boosting\", trained_model_gradient)\n",
    "\n",
    "\n",
    "# ===========================================================================\n",
    "# =========================== MODEL 5: SVM ==================================\n",
    "\n",
    "trained_model_svm = SVC()\n",
    "cross_validate_and_metrics(\"SVM\", trained_model_svm)\n",
    "\n",
    "\n",
    "# ===========================================================================\n",
    "# ============================ SOUMISSION ===================================\n",
    "\n",
    "# re-training before final predict\n",
    "# trained_model_gradient = GradientBoostingClassifier(criterion=\"friedman_mse\", learning_rate=0.2, loss=\"deviance\",\n",
    "#                                                     max_depth=3, max_features=\"sqrt\", min_samples_leaf=0.1,\n",
    "#                                                     min_samples_split=0.17273, n_estimators=10,\n",
    "#                                                     subsample=1).fit(X_train, y_train)\n",
    "\n",
    "trained_model_random_forest = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "                                                     max_depth=12, max_features='auto', max_leaf_nodes=None,\n",
    "                                                     min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                                                     min_samples_leaf=4, min_samples_split=10,\n",
    "                                                     min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=None,\n",
    "                                                     oob_score=False, random_state=None, verbose=0,\n",
    "                                                     warm_start=False).fit(X_train, y_train)\n",
    "\n",
    "trained_model_adaboost = AdaBoostClassifier().fit(X_train, y_train)\n",
    "\n",
    "y_pred_final = trained_model_adaboost.predict(X_test)\n",
    "submission = pd.DataFrame({\"PassengerId\": passengerId, \"Survived\": y_pred_final})\n",
    "\n",
    "filename = \"Titanic-Submission.csv\"\n",
    "submission.to_csv(filename, encoding=\"utf-8\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
