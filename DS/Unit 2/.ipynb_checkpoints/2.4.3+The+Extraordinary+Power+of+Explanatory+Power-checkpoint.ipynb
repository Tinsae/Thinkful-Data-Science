{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import warnings\n",
    "\n",
    "from IPython.display import display\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import linear_model\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Display preferences.\n",
    "%matplotlib inline\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "\n",
    "# Suppress annoying harmless error.\n",
    "warnings.filterwarnings(\n",
    "    action=\"ignore\",\n",
    "    module=\"scipy\",\n",
    "    message=\"^internal gelsd\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## The Extraordinary Power of Explanatory Power\n",
    "\n",
    "The strength of multiple linear regression lies in its ability to provide straightforward and interpretable solutions that not only predict future outcomes, but also provide insight into the underlying processes that create these outcomes.  For example, after fitting the following model:\n",
    "\n",
    "$$HourlyWidgetProduction = \\alpha + \\beta_1WorkerAgeFrom18+ \\beta_2WorkerYearsinJob + \\beta_3IsRoundWidget$$\n",
    "\n",
    "we get these parameters:\n",
    "$$\\alpha = 2$$\n",
    "$$\\beta_1 = .1$$\n",
    "$$\\beta_2 = .2$$\n",
    "$$\\beta_3 = 4$$\n",
    "\n",
    "Using those parameters, we learn that round widgets are twice as fast to produce as non-round widgets. We can tell because $\\alpha$ represents the intercept, the hourly rate of production for widgets that are not round (2 an hour) and $\\beta_3$ represents the difference between the intercept and the hourly rate of production for round widgets (also 2 an hour, for a total of 4 round widgets an hour).\n",
    "\n",
    "We also learn that for every year a worker ages after the age of 18, their hourly production-rate goes up by .1 ($\\beta_1$).  In addition, for every year a worker has been in that job, their hourly production-rate goes up by .2 ($\\beta_2$).  \n",
    "\n",
    "Furthermore, using this model, we can predict that a 20-year-old worker who has been in the job for a year and is making only round widgets will make $2 + .1*2 + .2*1 + 4 = 6.3$ round widgets an hour.\n",
    "\n",
    "Finally, and probably of greatest interest, we get an **R-Squared** value.  This is a proportion (between 0 and 1) that expresses how much variance in the outcome variable our model was able to explain.  Higher $R^2$ values are better to a point-- a low $R^2$ indicates that our model isn't explaining much information about the outcome, which means it will not give very good predictions.  However, a very high $R^2$ is a warning sign for overfitting.  No dataset is a perfect representation of reality, so a model that perfectly fits our data ($R^2$ of 1 or close to 1) is likely to be biased by quirks in the data, and will perform less well on the test-set.\n",
    "\n",
    "Here's an example using a toy advertising dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Acquire, load, and preview the data.\n",
    "data = pd.read_csv('https://tf-curricula-prod.s3.amazonaws.com/data-science/Advertising.csv')\n",
    "display(data.head())\n",
    "\n",
    "# Instantiate and fit our model.\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# from stackoverflow what is reshape(-1,1)\n",
    "#Now trying to reshape with (-1, 1) .\n",
    "#We have provided column as 1 but rows as unknown . So we get result new shape as (12, 1).again compatible with original shape(3,4)\n",
    "\n",
    "Y = data['Sales'].values.reshape(-1, 1)\n",
    "X = data[['TV','Radio','Newspaper']]\n",
    "regr.fit(X, Y)\n",
    "\n",
    "# Inspect the results.\n",
    "print('\\nCoefficients: \\n', regr.coef_)\n",
    "print('\\nIntercept: \\n', regr.intercept_)\n",
    "print('\\nR-squared:')\n",
    "print(regr.score(X, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "The model where the outcome Sales is predicted by the features TV, Radio, and Newspaper explains 89.7% of the variance in Sales.  Note that we don't know from these results how much of that variance is explained by each of the three features.  Looking at the coefficients, there appears to be a base rate of Sales that happen even with no ads in any medium (intercept: 2.939) and sales have the highest per-unit increase when ads are on the radio (0.189).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Assumptions of Multivariable Linear Regression\n",
    "\n",
    "For regression to work its magic, inputs to the model need to be consistent with four assumptions:\n",
    "\n",
    "\n",
    "### Assumption one: linear relationship\n",
    "\n",
    "As mentioned earlier, features in a regression need to have a linear relationship with the outcome.  If the relationship is non-linear, the regression model will try to find any hint of a linear relationship, and only explain that – with predictable consequences for the validity of the model.\n",
    "\n",
    "Sometimes this can be fixed by applying a non-linear transformation function to a feature.  For example, if the relationship between feature and outcome is quadratic and all feature scores are > 0, we can take the square root of the features, resulting in a linear relationship between the outcome and sqrt(feature).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Sample data.\n",
    "outcome = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "feature = [3, 4, 10, 16, 25, 33, 49, 60, 85, 100, 130, 140]\n",
    "\n",
    "# Plot the data as-is. Looks a mite quadratic.\n",
    "plt.scatter(outcome, feature)\n",
    "plt.title('Raw values')\n",
    "plt.show()\n",
    "\n",
    "# Create a feature using a non-linear transformation.\n",
    "sqrt_feature = [math.sqrt(x) for x in  feature]\n",
    "\n",
    "\n",
    "# Well now isn't that nice.\n",
    "plt.scatter(outcome, sqrt_feature)\n",
    "plt.title('Transformed values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "When interpreting features with non-linear transformations, it is important to keep the transformation in mind.  For example, in the equation $y = 2log({x})$, y increases by one unit for every two-unit increase in $log({x})$.  The relationship between y and x, however, is non-linear, and the amount of change in y varies based on the absolute value of x:\n",
    "\n",
    "|x\t|log(x)|\ty|\n",
    "|--|--|--|\n",
    "|1\t|0\t|0|\n",
    "|10\t|1\t|2|\n",
    "|100\t|2\t|4|\t\n",
    "|1000|\t3\t|6|\n",
    "\n",
    "So a one-unit change in x from 1 to 2 will result in a much greater change in y than a one-unit change in x from 100 to 101.\n",
    "\n",
    "There are many variable transformations.  For a deep dive, check out the Variable Linearization section of [Fifty Ways to Fix Your Data](https://statswithcats.wordpress.com/2010/11/21/fifty-ways-to-fix-your-data/).\n",
    "\n",
    "### Assumption two: multivariate normality\n",
    "\n",
    "The error from the model (calculated by subtracting the model-predicted values from the real outcome values) should be normally distributed.  Since ordinary least squares regression models are fitted by choosing the parameters that best minimize error, skewness or outliers in the error can result in serious miss-estimations.\n",
    "\n",
    "Outliers or skewness in error can often be traced back to outliers or skewness in data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Extract predicted values.\n",
    "predicted = regr.predict(X).ravel()\n",
    "actual = data['Sales']\n",
    "\n",
    "# Calculate the error, also called the residual.\n",
    "residual = actual - predicted\n",
    "\n",
    "# This looks a bit concerning.\n",
    "plt.hist(residual)\n",
    "plt.title('Residual counts')\n",
    "plt.xlabel('Residual')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "\n",
    "### Assumption three: homoscedasticity\n",
    "\n",
    "The distribution of your error terms (its \"scedasticity\"), should be consistent for all predicted values, or **homoscedastic**.\n",
    "\n",
    "For example, if your error terms aren't consistently distributed and you have more variance in the error for large outcome values than for small ones, then the confidence interval for large predicted values will be too small because it will be based on the average error variance.  This leads to overconfidence in the accuracy of your model's predictions.\n",
    "\n",
    "Some fixes to heteroscedasticity include transforming the dependent variable and adding features that target the poorly-estimated areas. For example, if a model tracks data over time and model error variance jumps in the September to November period, a binary feature indicating season may be enough to resolve the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(predicted, residual)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Residual')\n",
    "plt.axhline(y=0)\n",
    "plt.title('Residual vs. Predicted')\n",
    "plt.show()\n",
    "\n",
    "# Hm... looks a bit concerning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Assumption four: low multicollinearity\n",
    "\n",
    "Correlations among features should be low or nonexistent.  When features are correlated, they may both explain the same pattern of variance in the outcome.  The model will attempt to find a solution, potentially by attributing half the explanatory power to one feature and half to the other.  This isn’t a problem if our only goal is prediction, because then all that matters is that the variance gets explained.  However, if we want to know which features matter most when predicting an outcome, multicollinearity can cause us to underestimate the relationship between features and outcomes.\n",
    "\n",
    "Multicollinearity can be fixed by PCA or by discarding some of the correlated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "correlation_matrix = X.corr()\n",
    "display(correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drill: fixing assumptions\n",
    "\n",
    "Judging from the diagnostic plots, your data has a problem with both heteroscedasticity and multivariate non-normality.  Use the cell(s) below to see what you can do to fix it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Your code here.\n",
    "\n",
    "#heteroscedasticity\n",
    "actual\n",
    "#multivariate non-normality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "59px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
