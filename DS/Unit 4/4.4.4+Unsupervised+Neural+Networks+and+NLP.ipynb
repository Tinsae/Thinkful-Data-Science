{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "import nltk\n",
    "\n",
    "#!conda install -c anaconda --yes nltk \n",
    "#nltk.download('gutenberg')\n",
    "# !conda install -c anaconda --yes gensim \n",
    "#!pip install google-compute-engine\n",
    "#!conda install -c conda-forge google-cloud-sdk  --yes\n",
    "#!conda uninstall -c conda-forge google-cloud-sdk  --yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cracking Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall/NNP <--compound-- Street/NNP\n",
      "Street/NNP <--compound-- Journal/NNP\n",
      "Journal/NNP <--nsubj-- published/VBD\n",
      "just/RB <--advmod-- published/VBD\n",
      "published/VBD <--ROOT-- published/VBD\n",
      "an/DT <--det-- piece/NN\n",
      "interesting/JJ <--amod-- piece/NN\n",
      "piece/NN <--dobj-- published/VBD\n",
      "on/IN <--prep-- piece/NN\n",
      "crypto/JJ <--compound-- currencies/NNS\n",
      "currencies/NNS <--pobj-- on/IN\n",
      "./. <--punct-- published/VBD\n",
      "It/PRP <--nsubj-- talks/VBZ\n",
      "basically/RB <--advmod-- talks/VBZ\n",
      "talks/VBZ <--ROOT-- talks/VBZ\n",
      "about/IN <--prep-- talks/VBZ\n",
      "bitcoins/NNS <--pobj-- about/IN\n",
      "./. <--punct-- talks/VBZ\n"
     ]
    }
   ],
   "source": [
    "# dependcecy token by token\n",
    "nlp = spacy.load('en', disable=['ner'])\n",
    "\n",
    "doc = nlp('Wall Street Journal just published an interesting piece on crypto currencies. It basically talks about bitcoins.')\n",
    " \n",
    "for token in doc:\n",
    "    print(\"{0}/{1} <--{2}-- {3}/{4}\".format(\n",
    "        token.text, token.tag_, token.dep_, token.head.text, token.head.tag_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There/EX <--expl-- was/VBD\n",
      "was/VBD <--ROOT-- was/VBD\n",
      "nothing/NN <--attr-- was/VBD\n",
      "so/RB <--advmod-- remarkable/JJ\n",
      "very/RB <--advmod-- remarkable/JJ\n",
      "remarkable/JJ <--amod-- nothing/NN\n",
      "in/IN <--prep-- nothing/NN\n",
      "that/DT <--pobj-- in/IN\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en')\n",
    "\n",
    "doc = nlp('There was nothing so very remarkable in that')\n",
    " \n",
    "for token in doc:\n",
    "    print(\"{0}/{1} <--{2}-- {3}/{4}\".format(\n",
    "        token.text, token.tag_, token.dep_, token.head.text, token.head.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'encoding'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-adbd3402f500>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdisplacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstyle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'dep'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjupyter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'distance'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m90\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\displacy\\__init__.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(docs, style, page, minify, jupyter, options, manual)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mrenderer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconverter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfactories\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstyle\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mrenderer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[0mparsed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mconverter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmanual\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mdocs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m     \u001b[0m_html\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'parsed'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparsed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminify\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mminify\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[0mhtml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_html\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'parsed'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\displacy\\__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mrenderer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconverter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfactories\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstyle\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mrenderer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[0mparsed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mconverter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmanual\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mdocs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m     \u001b[0m_html\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'parsed'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparsed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminify\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mminify\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[0mhtml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_html\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'parsed'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\displacy\\__init__.py\u001b[0m in \u001b[0;36mparse_deps\u001b[1;34m(orig_doc, options)\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[0mRETURNS\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mGenerated\u001b[0m \u001b[0mdependency\u001b[0m \u001b[0mparse\u001b[0m \u001b[0mkeyed\u001b[0m \u001b[0mby\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0marcs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m     \"\"\"\n\u001b[1;32m---> 89\u001b[1;33m     \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDoc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_doc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_doc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_parsed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[0muser_warning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW005\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mdoc.pyx\u001b[0m in \u001b[0;36mspacy.tokens.doc.Doc.to_bytes\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mto_bytes\u001b[1;34m(getters, exclude)\u001b[0m\n\u001b[0;32m    484\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m             \u001b[0mserialized\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 486\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmsgpack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mserialized\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_bin_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    487\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\msgpack_numpy.py\u001b[0m in \u001b[0;36mpackb\u001b[1;34m(o, **kwargs)\u001b[0m\n\u001b[0;32m    194\u001b[0m     \"\"\"\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mPacker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0munpack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'encoding'"
     ]
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "spacy.displacy.render(doc, style='dep', jupyter=True, options={'distance': 90})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root:  published sentence: Wall Street Journal just published an interesting piece on crypto currencies.\n",
      "root:  talks sentence: It basically talks about bitcoins.\n",
      "\n",
      "Wall/NNP <--compound-- Street/NNP\n",
      "Street/NNP <--compound-- Journal/NNP\n",
      "Journal/NNP <--nsubj-- published/VBD\n",
      "just/RB <--advmod-- published/VBD\n",
      "published/VBD <--ROOT-- published/VBD\n",
      "an/DT <--det-- piece/NN\n",
      "interesting/JJ <--amod-- piece/NN\n",
      "piece/NN <--dobj-- published/VBD\n",
      "on/IN <--prep-- piece/NN\n",
      "crypto/JJ <--compound-- currencies/NNS\n",
      "currencies/NNS <--pobj-- on/IN\n",
      "./. <--punct-- published/VBD\n",
      "It/PRP <--nsubj-- talks/VBZ\n",
      "basically/RB <--advmod-- talks/VBZ\n",
      "talks/VBZ <--ROOT-- talks/VBZ\n",
      "about/IN <--prep-- talks/VBZ\n",
      "bitcoins/NNS <--pobj-- about/IN\n",
      "./. <--punct-- talks/VBZ\n"
     ]
    }
   ],
   "source": [
    "# dependcecy by sentence\n",
    "nlp = spacy.load('en', disable=['ner'])\n",
    "\n",
    "doc = nlp('Wall Street Journal just published an interesting piece on crypto currencies. It basically talks about bitcoins.')\n",
    " \n",
    "for sent in doc.sents:\n",
    "        print(\"root: \", sent.root, \"sentence:\", sent.text)\n",
    "print()\n",
    "for token in doc:\n",
    "    print(\"{0}/{1} <--{2}-- {3}/{4}\".format(\n",
    "        token.text, token.tag_, token.dep_, token.head.text, token.head.tag_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"0\" class=\"displacy\" width=\"1490\" height=\"272.0\" style=\"max-width: none; height: 272.0px; color: #000000; background: #ffffff; font-family: Arial\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Wall</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"140\">Street</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"140\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"230\">Journal</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"230\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"320\">just</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"320\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"410\">published</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"410\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"500\">an</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"500\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"590\">interesting</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"590\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"680\">piece</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"680\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"770\">on</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"770\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"860\">crypto</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"860\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"950\">currencies.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"950\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1040\">It</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1040\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1130\">basically</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1130\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1220\">talks</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1220\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1310\">about</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1310\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"182.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1400\">bitcoins.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1400\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-0\" stroke-width=\"2px\" d=\"M70,137.0 C70,92.0 130.0,92.0 130.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-0\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,139.0 L62,127.0 78,127.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-1\" stroke-width=\"2px\" d=\"M160,137.0 C160,92.0 220.0,92.0 220.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-1\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M160,139.0 L152,127.0 168,127.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-2\" stroke-width=\"2px\" d=\"M250,137.0 C250,47.0 405.0,47.0 405.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-2\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M250,139.0 L242,127.0 258,127.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-3\" stroke-width=\"2px\" d=\"M340,137.0 C340,92.0 400.0,92.0 400.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-3\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M340,139.0 L332,127.0 348,127.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-4\" stroke-width=\"2px\" d=\"M520,137.0 C520,47.0 675.0,47.0 675.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-4\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M520,139.0 L512,127.0 528,127.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-5\" stroke-width=\"2px\" d=\"M610,137.0 C610,92.0 670.0,92.0 670.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-5\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M610,139.0 L602,127.0 618,127.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-6\" stroke-width=\"2px\" d=\"M430,137.0 C430,2.0 680.0,2.0 680.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-6\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M680.0,139.0 L688.0,127.0 672.0,127.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-7\" stroke-width=\"2px\" d=\"M700,137.0 C700,92.0 760.0,92.0 760.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-7\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M760.0,139.0 L768.0,127.0 752.0,127.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-8\" stroke-width=\"2px\" d=\"M880,137.0 C880,92.0 940.0,92.0 940.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-8\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M880,139.0 L872,127.0 888,127.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-9\" stroke-width=\"2px\" d=\"M790,137.0 C790,47.0 945.0,47.0 945.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-9\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945.0,139.0 L953.0,127.0 937.0,127.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-10\" stroke-width=\"2px\" d=\"M1060,137.0 C1060,47.0 1215.0,47.0 1215.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-10\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1060,139.0 L1052,127.0 1068,127.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-11\" stroke-width=\"2px\" d=\"M1150,137.0 C1150,92.0 1210.0,92.0 1210.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-11\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1150,139.0 L1142,127.0 1158,127.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-12\" stroke-width=\"2px\" d=\"M1240,137.0 C1240,92.0 1300.0,92.0 1300.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-12\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1300.0,139.0 L1308.0,127.0 1292.0,127.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-13\" stroke-width=\"2px\" d=\"M1330,137.0 C1330,92.0 1390.0,92.0 1390.0,137.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-13\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1390.0,139.0 L1398.0,127.0 1382.0,127.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # plotting the graph\n",
    "from spacy import displacy\n",
    " \n",
    "displacy.render(doc, style='dep', jupyter=True, options={'distance': 90})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Intro to word2vec\n",
    "\n",
    "The most common unsupervised neural network approach for NLP is word2vec, a shallow neural network model for converting words to vectors using distributed representation: Each word is represented by many neurons, and each neuron is involved in representing many words.  At the highest level of abstraction, word2vec assigns a vector of random values to each word.  For a word W, it looks at the words that are near W in the sentence, and shifts the values in the word vectors such that the vectors for words near that W are closer to the W vector, and vectors for words not near W are farther away from the W vector.  With a large enough corpus, this will eventually result in words that often appear together having vectors that are near one another, and words that rarely or never appear together having vectors that are far away from each other.  Then, using the vectors, similarity scores can be computed for each pair of words by taking the cosine of the vectors.  \n",
    "\n",
    "This may sound quite similar to the Latent Semantic Analysis approach you just learned.  The conceptual difference is that LSA creates vector representations of sentences based on the words in them, while word2vec creates representations of individual words, based on the words around them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is it good for?\n",
    "\n",
    "Word2vec is useful for any time when computers need to parse requests written by humans. The problem with human communication is that there are so many different ways to communicate the same concept. It's easy for us, as humans, to know that \"the silverware\" and \"the utensils\" can refer to the same thing. Computers can't do that unless we teach them, and this can be a real chokepoint for human/computer interactions. If you've ever played a text adventure game (think _Colossal Cave Adventure_ or _Zork_), you may have encountered the following scenario:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "GAME: You are on a forest path north of the field. A cave leads into a granite butte to the north.\n",
    "A thick hedge blocks the way to the west.\n",
    "A hefty stick lies on the ground.\n",
    "\n",
    "YOU: pick up stick  \n",
    "\n",
    "GAME: You don't know how to do that.  \n",
    "\n",
    "YOU: lift stick  \n",
    "\n",
    "GAME: You don't know how to do that.  \n",
    "\n",
    "YOU: take stick  \n",
    "\n",
    "GAME: You don't know how to do that.  \n",
    "\n",
    "YOU: grab stick  \n",
    "\n",
    "GAME: You grab the stick from the ground and put it in your bag.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And your brain explodes from frustration. A text adventure game that incorporates a properly trained word2vec model would have vectors for \"pick up\", \"lift\", and \"take\" that are close to the vector for \"grab\" and therefore could accept those other verbs as synonyms so you could move ahead faster. In more practical applications, word2vec and other similar algorithms are what help a search engine return the best results for your query and not just the ones that contain the exact words you used. In fact, search is a better example, because not only does the search engine need to understand your request, it also needs to match it to web pages that were _also written by humans_ and therefore _also use idiosyncratic language_.\n",
    "\n",
    "Humans, man.  \n",
    "\n",
    "So how does it work?\n",
    "\n",
    "## Generating vectors: Multiple algorithms\n",
    "\n",
    "In considering the relationship between a word and its surrounding words, word2vec has two options that are the inverse of one another:\n",
    "\n",
    " * _Continuous Bag of Words_ (CBOW): the identity of a word is predicted using the words near it in a sentence.\n",
    " * _Skip-gram_: The identities of words are predicted from the word they surround. Skip-gram seems to work better for larger corpuses.\n",
    "\n",
    "For the sentence \"Terry Gilliam is a better comedian than a director\", if we focus on the word \"comedian\" then CBOW will try to predict \"comedian\" using \"is\", \"a\", \"better\", \"than\", \"a\", and \"director\".  Skip-gram will try to predict \"is\", \"a\", \"better\", \"than\", \"a\", and \"director\" using the word \"comedian\". In practice, for CBOW the vector for \"comedian\" will be pulled closer to the other words, while for skip-gram the vectors for the other words will be pulled closer to \"comedian\".  \n",
    "\n",
    "In addition to moving the vectors for nearby words closer together, each time a word is processed some vectors are moved farther away. Word2vec has two approaches to \"pushing\" vectors apart:\n",
    " \n",
    " * _Negative sampling_: Like it says on the tin, each time a word is pulled toward some neighbors, the vectors for a randomly chosen small set of other words are pushed away.\n",
    " * _Hierarchical softmax_: Every neighboring word is pulled closer or farther from a subset of words chosen based on a tree of probabilities.\n",
    "\n",
    "## What is similarity? Word2vec strengths and weaknesses\n",
    "\n",
    "Keep in mind that word2vec operates on the assumption that frequent proximity indicates similarity, but words can be \"similar\" in various ways. They may be conceptually similar (\"royal\", \"king\", and \"throne\"), but they may also be functionally similar (\"tremendous\" and \"negligible\" are both common modifiers of \"size\"). Here is a more detailed exploration, [with examples](https://quomodocumque.wordpress.com/2016/01/15/messing-around-with-word2vec/), of what \"similarity\" means in word2vec.\n",
    "\n",
    "One cool thing about word2vec is that it can identify similarities between words _that never occur near one another in the corpus_. For example, consider these sentences:\n",
    "\n",
    "\"The dog played with an elastic ball.\"\n",
    "\"Babies prefer the ball that is bouncy.\"\n",
    "\"I wanted to find a ball that's elastic.\"\n",
    "\"Tracy threw a bouncy ball.\"\n",
    "\n",
    "\"Elastic\" and \"bouncy\" are similar in meaning in the text but don't appear in the same sentence. However, both appear near \"ball\". In the process of nudging the vectors around so that \"elastic\" and \"bouncy\" are both near the vector for \"ball\", the words also become nearer to one another and their similarity can be detected.\n",
    "\n",
    "For a while after it was introduced, [no one was really sure why word2vec worked as well as it did](https://arxiv.org/pdf/1402.3722v1.pdf) (see last paragraph of the linked paper). A few years later, some additional math was developed to explain word2vec and similar models. If you are comfortable with both math and \"academese\", have a lot of time on your hands, and want to take a deep dive into the inner workings of word2vec, [check out this paper](https://arxiv.org/pdf/1502.03520v7.pdf) from 2016.  \n",
    "\n",
    "One of the draws of word2vec when it first came out was that the vectors could be used to convert analogies (\"king\" is to \"queen\" as \"man\" is to \"woman\", for example) into mathematical expressions (\"king\" + \"woman\" - \"man\" = ?) and solve for the missing element (\"queen\"). This is kinda nifty.\n",
    "\n",
    "A drawback of word2vec is that it works best with a corpus that is at least several billion words long. Even though the word2vec algorithm is speedy, this is a a lot of data and takes a long time! Our example dataset is only two million words long, which allows us to run it in the notebook without overwhelming the kernel, but probably won't give great results.  Still, let's try it!\n",
    "\n",
    "There are a few word2vec implementations in Python, but the general consensus is the easiest one to us is in [gensim](https://radimrehurek.com/gensim/models/word2vec.html). Now is a good time to `pip install gensim` if you don't have it yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to clean text.\n",
    "def text_cleaner(text):\n",
    "    \n",
    "    # Visual inspection shows spaCy does not recognize the double dash '--'.\n",
    "    # Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    \n",
    "    # Get rid of headings in square brackets.\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    \n",
    "    # Get rid of chapter titles.\n",
    "    text = re.sub(r'Chapter \\d+','',text)\n",
    "    \n",
    "    # Get rid of extra whitespace.\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This gives a very long text. spacy has a limit\n",
    "# # Import all the Austen in the Project Gutenberg corpus.\n",
    "# austen = \"\"\n",
    "# for novel in ['persuasion','emma','sense']:\n",
    "#     work = gutenberg.raw('austen-' + novel + '.txt')\n",
    "#     austen = austen + work\n",
    "\n",
    "# # Clean the data.\n",
    "# austen_clean = text_cleaner(austen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the data. This can take some time.\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do separetly\n",
    "persuasion = text_cleaner(gutenberg.raw('austen-persuasion.txt'))\n",
    "emma = text_cleaner(gutenberg.raw('austen-emma.txt'))\n",
    "sense = text_cleaner(gutenberg.raw('austen-sense.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "462818\n",
      "876869\n",
      "666583\n"
     ]
    }
   ],
   "source": [
    "print(len(persuasion))\n",
    "print(len(emma))\n",
    "print(len(sense))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doesn't work. kernel crashes or complains about memory limit\n",
    "#austen_doc = nlp(austen_clean)\n",
    "# do it separetly\n",
    "persuasion_doc = nlp(persuasion)\n",
    "emma_doc = nlp(emma)\n",
    "sense_doc = nlp(sense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'daughter', 'eld', 'would', 'really', 'give', 'thing', 'much', 'tempt']\n",
      "We have 17853 sentences and 2006270 tokens.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# Organize the parsed doc into sentences, while filtering out punctuation\n",
    "# and stop words, and converting words to lower case lemmas.\n",
    "sentences = []\n",
    "for austen_doc in (persuasion_doc, emma_doc, sense_doc):\n",
    "    for sentence in austen_doc.sents:\n",
    "        sentence = [\n",
    "            token.lemma_.lower()\n",
    "            for token in sentence\n",
    "            if not token.lemma_.lower() in stopwords.words('english')\n",
    "            and not token.is_punct\n",
    "            and token.lemma_ != \"-PRON-\"\n",
    "        ]\n",
    "        sentences.append(sentence)\n",
    "\n",
    "print(sentences[20])\n",
    "print('We have {} sentences and {} tokens.'.format(len(sentences), len(persuasion) + len(emma) + len(sense)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-21 19:43:01,070 : INFO : 'pattern' package not found; tag filters are not available for English\n",
      "2019-02-21 19:43:01,079 : INFO : collecting all words and their counts\n",
      "2019-02-21 19:43:01,080 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-02-21 19:43:01,105 : INFO : PROGRESS: at sentence #10000, processed 92243 words, keeping 6042 word types\n",
      "2019-02-21 19:43:01,126 : INFO : collected 7603 word types from a corpus of 166833 raw words and 17853 sentences\n",
      "2019-02-21 19:43:01,127 : INFO : Loading a fresh vocabulary\n",
      "2019-02-21 19:43:01,137 : INFO : effective_min_count=10 retains 2021 unique words (26% of original 7603, drops 5582)\n",
      "2019-02-21 19:43:01,138 : INFO : effective_min_count=10 leaves 152265 word corpus (91% of original 166833, drops 14568)\n",
      "2019-02-21 19:43:01,146 : INFO : deleting the raw counts dictionary of 7603 items\n",
      "2019-02-21 19:43:01,148 : INFO : sample=0.001 downsamples 61 most-common words\n",
      "2019-02-21 19:43:01,149 : INFO : downsampling leaves estimated 135491 word corpus (89.0% of prior 152265)\n",
      "2019-02-21 19:43:01,153 : INFO : constructing a huffman tree from 2021 words\n",
      "2019-02-21 19:43:01,202 : INFO : built huffman tree with maximum node depth 14\n",
      "2019-02-21 19:43:01,208 : INFO : estimated required memory for 2021 words and 300 dimensions: 8690300 bytes\n",
      "2019-02-21 19:43:01,209 : INFO : resetting layer weights\n",
      "2019-02-21 19:43:01,242 : INFO : training model with 4 workers on 2021 vocabulary and 300 features, using sg=0 hs=1 sample=0.001 negative=5 window=6\n",
      "2019-02-21 19:43:01,610 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-21 19:43:01,619 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-21 19:43:01,631 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-21 19:43:01,642 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-21 19:43:01,643 : INFO : EPOCH - 1 : training on 166833 raw words (135410 effective words) took 0.4s, 346225 effective words/s\n",
      "2019-02-21 19:43:02,031 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-21 19:43:02,041 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-21 19:43:02,056 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-21 19:43:02,058 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-21 19:43:02,060 : INFO : EPOCH - 2 : training on 166833 raw words (135357 effective words) took 0.4s, 330486 effective words/s\n",
      "2019-02-21 19:43:02,450 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-21 19:43:02,489 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-21 19:43:02,502 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-21 19:43:02,504 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-21 19:43:02,505 : INFO : EPOCH - 3 : training on 166833 raw words (135521 effective words) took 0.4s, 323115 effective words/s\n",
      "2019-02-21 19:43:02,914 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-21 19:43:02,940 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-21 19:43:02,948 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-21 19:43:02,955 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-21 19:43:02,956 : INFO : EPOCH - 4 : training on 166833 raw words (135397 effective words) took 0.4s, 307136 effective words/s\n",
      "2019-02-21 19:43:03,319 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-21 19:43:03,326 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-21 19:43:03,338 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-21 19:43:03,354 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-21 19:43:03,354 : INFO : EPOCH - 5 : training on 166833 raw words (135420 effective words) took 0.4s, 347412 effective words/s\n",
      "2019-02-21 19:43:03,355 : INFO : training on a 834165 raw words (677105 effective words) took 2.1s, 320584 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "\n",
    "model = word2vec.Word2Vec(\n",
    "    sentences,\n",
    "    workers=4,     # Number of threads to run in parallel (if your computer does parallel processing).\n",
    "    min_count=10,  # Minimum word count threshold.\n",
    "    window=6,      # Number of words around target word to consider.\n",
    "    sg=0,          # Use CBOW because our corpus is small.\n",
    "    sample=1e-3 ,  # Penalize frequent words.\n",
    "    size=300,      # Word vector length.\n",
    "    hs=1           # Use hierarchical softmax.\n",
    ")\n",
    "\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01862508, -0.11993532, -0.13501856,  0.07445867,  0.03582802,\n",
       "        0.00733998, -0.06704213,  0.09933243,  0.04451099, -0.05703803],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see sample word vector\n",
    "model.wv[\"dance\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-21 19:43:03,376 : INFO : precomputing L2-norms of word weight vectors\n",
      "/home/tigial3535/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
      "  if sys.path[0] == '':\n",
      "2019-02-21 19:43:03,386 : WARNING : vectors for words {'lunch'} are not present in the model, ignoring these words\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('attention', 0.6234357953071594), ('daughter', 0.6037120819091797), ('people', 0.5309017896652222), ('friend', 0.5290405750274658), ('satisfaction', 0.49827444553375244), ('indisposition', 0.49275344610214233), ('able', 0.4799255132675171), ('pleasing', 0.47090378403663635), ('address', 0.46492719650268555), ('father', 0.4638645648956299)]\n",
      "0.8242786\n",
      "0.331333\n",
      "marriage\n"
     ]
    }
   ],
   "source": [
    "# List of words in model.\n",
    "vocab = model.wv.vocab.keys()\n",
    "\n",
    "print(model.wv.most_similar(positive=['lady', 'man'], negative=['woman']))\n",
    "\n",
    "# Similarity is calculated using the cosine, so again 1 is total\n",
    "# similarity and 0 is no similarity.\n",
    "print(model.wv.similarity('loud', 'aloud'))\n",
    "print(model.wv.similarity('mr', 'mrs'))\n",
    "\n",
    "# One of these things is not like the other...\n",
    "print(model.doesnt_match(\"breakfast marriage dinner lunch\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Clearly this model is not great â€“ while some words given above might possibly fill in the analogy woman:lady::man:?, most answers likely make little sense. You'll notice as well that re-running the model likely gives you different results, indicating random chance plays a large role here.\n",
    "\n",
    "We do, however, get a nice result on \"marriage\" being dissimilar to \"breakfast\", \"lunch\", and \"dinner\". \n",
    "\n",
    "## Drill 0\n",
    "\n",
    "Take a few minutes to modify the hyperparameters of this model and see how its answers change. Can you wrangle any improvements?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-21 19:43:03,397 : INFO : collecting all words and their counts\n",
      "2019-02-21 19:43:03,398 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-02-21 19:43:03,419 : INFO : PROGRESS: at sentence #10000, processed 92243 words, keeping 6042 word types\n",
      "2019-02-21 19:43:03,434 : INFO : collected 7603 word types from a corpus of 166833 raw words and 17853 sentences\n",
      "2019-02-21 19:43:03,435 : INFO : Loading a fresh vocabulary\n",
      "2019-02-21 19:43:03,444 : INFO : effective_min_count=5 retains 3017 unique words (39% of original 7603, drops 4586)\n",
      "2019-02-21 19:43:03,445 : INFO : effective_min_count=5 leaves 158867 word corpus (95% of original 166833, drops 7966)\n",
      "2019-02-21 19:43:03,457 : INFO : deleting the raw counts dictionary of 7603 items\n",
      "2019-02-21 19:43:03,459 : INFO : sample=0.001 downsamples 58 most-common words\n",
      "2019-02-21 19:43:03,459 : INFO : downsampling leaves estimated 142915 word corpus (90.0% of prior 158867)\n",
      "2019-02-21 19:43:03,464 : INFO : constructing a huffman tree from 3017 words\n",
      "2019-02-21 19:43:03,538 : INFO : built huffman tree with maximum node depth 15\n",
      "2019-02-21 19:43:03,546 : INFO : estimated required memory for 3017 words and 300 dimensions: 12973100 bytes\n",
      "2019-02-21 19:43:03,547 : INFO : resetting layer weights\n",
      "2019-02-21 19:43:03,592 : INFO : training model with 4 workers on 3017 vocabulary and 300 features, using sg=0 hs=1 sample=0.001 negative=5 window=10\n",
      "2019-02-21 19:43:04,002 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-21 19:43:04,032 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-21 19:43:04,037 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-21 19:43:04,041 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-21 19:43:04,042 : INFO : EPOCH - 1 : training on 166833 raw words (142932 effective words) took 0.4s, 324219 effective words/s\n",
      "2019-02-21 19:43:04,458 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-21 19:43:04,468 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-21 19:43:04,476 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-21 19:43:04,491 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-21 19:43:04,492 : INFO : EPOCH - 2 : training on 166833 raw words (143079 effective words) took 0.4s, 323463 effective words/s\n",
      "2019-02-21 19:43:04,902 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-21 19:43:04,925 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-21 19:43:04,931 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-21 19:43:04,943 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-21 19:43:04,944 : INFO : EPOCH - 3 : training on 166833 raw words (142989 effective words) took 0.4s, 321788 effective words/s\n",
      "2019-02-21 19:43:05,357 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-21 19:43:05,370 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-21 19:43:05,382 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-21 19:43:05,395 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-21 19:43:05,396 : INFO : EPOCH - 4 : training on 166833 raw words (142880 effective words) took 0.4s, 321704 effective words/s\n",
      "2019-02-21 19:43:05,808 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-02-21 19:43:05,825 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-02-21 19:43:05,832 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-02-21 19:43:05,846 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-02-21 19:43:05,848 : INFO : EPOCH - 5 : training on 166833 raw words (142956 effective words) took 0.4s, 323034 effective words/s\n",
      "2019-02-21 19:43:05,848 : INFO : training on a 834165 raw words (714836 effective words) took 2.3s, 316988 effective words/s\n",
      "2019-02-21 19:43:05,856 : INFO : precomputing L2-norms of word weight vectors\n",
      "/home/tigial3535/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:23: DeprecationWarning: Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
      "2019-02-21 19:43:05,863 : WARNING : vectors for words {'lunch'} are not present in the model, ignoring these words\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('observant', 0.5732364654541016), ('entertain', 0.5313925743103027), ('associate', 0.5287098288536072), ('daughter', 0.5250798463821411), ('train', 0.5061901211738586), ('congratulate', 0.5046765804290771), ('handsome', 0.5040509700775146), ('hysteric', 0.49939125776290894), ('back', 0.4969525635242462), ('row', 0.48633307218551636)]\n",
      "0.6655203\n",
      "0.43741384\n",
      "marriage\n"
     ]
    }
   ],
   "source": [
    "# Tinker with hyperparameters here.\n",
    "model = word2vec.Word2Vec(\n",
    "    sentences,\n",
    "    workers=4,     # Number of threads to run in parallel (if your computer does parallel processing).\n",
    "    min_count=5,  # Minimum word count threshold.\n",
    "    window=10,      # Number of words around target word to consider.\n",
    "    sg=0,          # Use CBOW because our corpus is small.\n",
    "    sample=1e-3 ,  # Penalize frequent words.\n",
    "    size=300,      # Word vector length.\n",
    "    hs=1           # Use hierarchical softmax.\n",
    ")\n",
    "# List of words in model.\n",
    "vocab = model.wv.vocab.keys()\n",
    "\n",
    "print(model.wv.most_similar(positive=['lady', 'man'], negative=['woman']))\n",
    "\n",
    "# Similarity is calculated using the cosine, so again 1 is total\n",
    "# similarity and 0 is no similarity.\n",
    "print(model.wv.similarity('loud', 'aloud'))\n",
    "print(model.wv.similarity('mr', 'mrs'))\n",
    "\n",
    "# One of these things is not like the other...\n",
    "print(model.doesnt_match(\"breakfast marriage dinner lunch\".split()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# Example word2vec applications\n",
    "\n",
    "You can use the vectors from word2vec as features in other models, or try to gain insight from the vector compositions themselves.\n",
    "\n",
    "Here are some neat things people have done with word2vec:\n",
    "\n",
    " * [Visualizing word embeddings in Jane Austen's Pride and Prejudice](http://blogger.ghostweather.com/2014/11/visualizing-word-embeddings-in-pride.html). Skip to the bottom to see a _truly honest_ account of this data scientist's process.\n",
    "\n",
    " * [Tracking changes in Dutch Newspapers' associations with words like 'propaganda' and 'alien' from 1950 to 1990](https://www.slideshare.net/MelvinWevers/concepts-through-time-tracing-concepts-in-dutch-newspaper-discourse-using-sequential-word-vector-spaces).\n",
    "\n",
    " * [Helping customers find clothing items similar to a given item but differing on one or more characteristics](http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drill 1: Word2Vec on 100B+ words\n",
    "\n",
    "As we mentioned, word2vec really works best on a big corpus, but it can take half a day to clean such a corpus and run word2vec on it.  Fortunately, there are word2vec models available that have already been trained on _really_ big corpora. They are big files, but you can download a [pretrained model of your choice here](https://github.com/3Top/word2vec-api). At minimum, the ones built with word2vec (check the \"Architecture\" column) should load smoothly using an appropriately modified version of the code below, and you can play to your heart's content.\n",
    "\n",
    "Because the models are so large, however, you may run into memory problems or crash the kernel. If you can't get a pretrained model to run locally, check out this [interactive web app of the Google News model](https://rare-technologies.com/word2vec-tutorial/#bonus_app) instead.\n",
    "\n",
    "However you access it, play around with a pretrained model. Is there anything interesting you're able to pull out about analogies, similar words, or words that don't match? Write up a quick note about your tinkering and discuss it with your mentor during your next session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-21 19:44:26,896 : INFO : loading projection weights from /home/tigial3535/google_word_vectors.bin.gz\n",
      "2019-02-21 19:46:28,112 : INFO : loaded (3000000, 300) matrix from /home/tigial3535/google_word_vectors.bin.gz\n"
     ]
    }
   ],
   "source": [
    "# Load Google's pre-trained Word2Vec model.\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format ('/home/tigial3535/google_word_vectors.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tigial3535/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n",
      "2019-02-21 19:47:04,840 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('fella', 0.6031545400619507), ('gentleman', 0.5849649906158447), ('chap', 0.5543248653411865), ('gent', 0.543907880783081), ('guy', 0.5265033841133118), ('lad', 0.5139425992965698), ('feller', 0.5072450041770935), ('bloke', 0.49030160903930664), ('rascal', 0.4873698949813843), ('ladies', 0.47617611289024353)]\n"
     ]
    }
   ],
   "source": [
    "# Play around with your pretrained model here.\n",
    "print(model.wv.most_similar(positive=['lady', 'man'], negative=['woman']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much better result.if lady is to man, gentelman is to women. fela and guy are also good words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tigial3535/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('services', 0.5195369720458984), ('ser_vice', 0.47518986463546753), ('service.The', 0.4567696452140808), ('sevice', 0.4489085078239441), ('places', 0.4361933469772339), ('facilities_distinguishes_EarthSearch', 0.4195823669433594), ('Complimentary_valet', 0.4184284806251526), ('servive', 0.41484370827674866), ('Princeton_Wendy_Benchley', 0.4123907685279846), ('Starbuck_Lind_Mortuary', 0.4115825891494751)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar(positive=['place', 'service']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It caught words with slight spelling mistakes. This will be good to analyze reviews with many typos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tigial3535/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('palce', 0.3032969534397125), ('1st_SOPS', 0.2987600862979889), ('sevice', 0.29494941234588623), ('Cablevision_Optimum_Lightpath', 0.2813442349433899), ('ILS_Proton_launch', 0.27688130736351013), ('#oo#', 0.27500271797180176), ('Khokhrapar_Munabao_train', 0.27090984582901), ('BigPond_Broadband', 0.26873475313186646), ('doctor_Lothar_Heinrich', 0.2677452266216278), ('Euro_PacketCable', 0.2674042284488678)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar(positive=['place', 'service'],  negative=['restaurant', 'food']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tigial3535/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Milan', 0.7222141027450562), ('Rome', 0.7028310298919678), ('Palermo_Sicily', 0.5967570543289185), ('Italian', 0.5911272764205933), ('Tuscany', 0.5632812976837158), ('Bologna', 0.5608358383178711), ('Sicily', 0.5596384406089783), ('Bologna_Italy', 0.5470059514045715), ('Berna_Milan', 0.5464028120040894), ('Genoa', 0.5308899879455566)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar(positive=['Paris', 'Italy'],  negative=['France']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tigial3535/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Sworn_Enemy', 0.6559710502624512), ('Hinder_Papa_Roach', 0.6507534980773926), ('KILL_HANNAH', 0.6474124193191528), ('Singer_Jacoby_Shaddix', 0.645458459854126), ('Gracious_Few', 0.644819974899292), ('Buckcherry_Papa_Roach', 0.6439394950866699), ('Drummer_Quits', 0.6436043977737427), ('Jonny_Lives', 0.6390442848205566), ('Damned_Things', 0.6344761848449707), ('Duff_McKagan_Loaded', 0.6319283246994019)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar(positive=['2Cents']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It found services others than restaurants and food."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technical Coaching\n",
    "\n",
    "Tiago [4 days ago]\n",
    "Hello, Tinsae. I'm taking a look at stack overflow. Can you share with me the problematic part of the code and the dataset?\n",
    "\n",
    "\n",
    "Tinsae [4 days ago]\n",
    "4.4.4+Unsupervised+Neural+Networks+and+NLP\n",
    "\n",
    "Input 5\n",
    "\n",
    "\n",
    "Tinsae [4 days ago]\n",
    "spacyerror.png\n",
    "\n",
    "\n",
    "\n",
    "Tiago [4 days ago]\n",
    "How many GBs of RAM do you have available?\n",
    "\n",
    "\n",
    "Tinsae [4 days ago]\n",
    "It is 8 GB computer. I assume at least 5 GB is free\n",
    "raminfo.png\n",
    "\n",
    "\n",
    "\n",
    "Tiago [4 days ago]\n",
    "can you take a look at task manager, please?\n",
    "\n",
    "Tiago [4 days ago]\n",
    "you're using windows, right?\n",
    "\n",
    "\n",
    "Tinsae [4 days ago]\n",
    "yes\n",
    "\n",
    "\n",
    "Tiago [4 days ago]\n",
    "meanwhile, I'm trying to run the same thing in google colab, just to give us an environment with tons of RAM to spare\n",
    "\n",
    "\n",
    "Tiago [4 days ago]\n",
    "Tinsae, I am being able to run it in google colab with the following modification:\n",
    "\n",
    "```# Parse the data. This can take some time.\n",
    "nlp = spacy.load('en')\n",
    "nlp.max_length = 2006272 + 1\n",
    "austen_doc = nlp(austen_clean)```\n",
    "\n",
    "\n",
    "Tiago [4 days ago]\n",
    "aaaand my session crashed after using all available RAM\n",
    "\n",
    "\n",
    "Tinsae [4 days ago]\n",
    "How much ram did Google Colab gave you?\n",
    "\n",
    "\n",
    "Tiago [4 days ago]\n",
    "about 12GB\n",
    "\n",
    "\n",
    "Tinsae [4 days ago]\n",
    "I have 56 GB ram VM on Google Cloud Platform\n",
    "\n",
    "\n",
    "Tiago [4 days ago]\n",
    "btw, 12GB free\n",
    "\n",
    "\n",
    "Tinsae [4 days ago]\n",
    "I am using the free $300\n",
    "\n",
    "\n",
    "Tiago [4 days ago]\n",
    "that might work. But I want to see if we are able to break that work in chunks\n",
    "\n",
    "Tiago [4 days ago]\n",
    "and make it less memory-hungry\n",
    "\n",
    "\n",
    "Tiago [4 days ago]\n",
    "(another way would be to make a swapfile, but performance will be awful)\n",
    "\n",
    "\n",
    "Tinsae [4 days ago]\n",
    "Yeah, making it less memory hungry is what am also aiming.\n",
    "\n",
    "\n",
    "Tiago [4 days ago]\n",
    "a dumb way to solve it is to use spacy 1.x\n",
    "\n",
    "Tiago [4 days ago]\n",
    "I'll try that first\n",
    "\n",
    "\n",
    "Tiago [4 days ago]\n",
    "you using anaconda or pure python?\n",
    "\n",
    "\n",
    "Tinsae [4 days ago]\n",
    "anaconda\n",
    "\n",
    "\n",
    "Tiago [4 days ago]\n",
    "okay, it works under spacy 1.x\n",
    "\n",
    "\n",
    "Tiago [4 days ago]\n",
    "let me give you the command to do a conda install of spacy 1.x\n",
    "\n",
    "\n",
    "Tiago [4 days ago]\n",
    "one moment, as I figure this out\n",
    "\n",
    "\n",
    "Tiago [4 days ago]\n",
    "got it. You have to open a prompt and type:\n",
    "\n",
    "```conda install 'spacy<2'```\n",
    "\n",
    "\n",
    "Tiago [4 days ago]\n",
    "and now, let me talk in the mentor channel as how we update the curriculum\n",
    "\n",
    "\n",
    "Tinsae [4 days ago]\n",
    "Is that downgrading spacy?\n",
    "\n",
    "\n",
    "Tiago [4 days ago]\n",
    "yes\n",
    "\n",
    "\n",
    "Tinsae [4 days ago]\n",
    "ok. That is one way to solve it\n",
    "\n",
    "\n",
    "Tiago [4 days ago]\n",
    "yes, that's the \"dumb\" way. The other one is to figure out the pieces of the pipeline (which you can do by typing `nlp.pipeline` in the notebook) and checking whether they can run in chunks of data\n",
    "\n",
    "\n",
    "Tiago [4 days ago]\n",
    "I'm going to give it a try, but, as it is no longer high-priority, if other tickets arise, I'll have to stop to deal with them, okay?\n",
    "\n",
    "Tinsae [4 days ago]\n",
    "ok. It is solved ticket\n",
    "\n",
    "\n",
    "Tinsae [4 days ago]\n",
    "Thanks Tiago\n",
    "\n",
    "\n",
    "Tiago [4 days ago]\n",
    "I'll spend some more time with it before marking it as solved, okay?\n",
    "\n",
    "\n",
    "Tinsae [4 days ago]\n",
    "ok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technical Coaching 2\n",
    "\n",
    " Requested by @Tinsae G. Alemayehu\n",
    "*import gensim\n",
    "creates\n",
    "module 'boto' has no attribute 'plugin'\n",
    "error. How can I solve it?*\n",
    "\n",
    "â€¢ What have you tried so far?\n",
    "\n",
    "Tinsae   [1 minute ago]\n",
    "Solved it!!\n",
    "\n",
    "Tinsae   [1 minute ago]\n",
    "I removed a virtual machine which had prebuilt libraries like tensorflow, sklearn on Debian OS. I installed a new Ubuntu image and installed anaconda from scratch. Ubuntu has more support on stack exchange.\n",
    "\n",
    "I uninstalled gensim which was insalled by conda and used pip\n",
    "\n",
    "pip install gensim\n",
    "\n",
    "pip install google_cloud_platform\n",
    "\n",
    "pip install --upgrade gensim smart_open\n",
    "\n",
    "finally I restarted the kernel and it worked!!\n",
    "\n",
    "Ticket Solved! (edited)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Techical Chocing three\n",
    "\n",
    "\n",
    "Tiago   [7 hours ago]\n",
    "Hey, Tinsae. You have 5 billion user reviews, like, actual reviews? Wow. Where did you get that?\n",
    "\n",
    "Tiago   [7 hours ago]\n",
    "And let me take a look at the Google News Corpus.\n",
    "\n",
    "Tinsae   [7 hours ago]\n",
    "Sorry I meant 5 million :slightly_smiling_face:\n",
    "\n",
    "Tiago   [7 hours ago]\n",
    "But unfortunately I have no specific answer for you, I don't know what would be a good size.\n",
    "\n",
    "Tinsae   [7 hours ago]\n",
    "you may check this\n",
    "https://github.com/3Top/word2vec-api\n",
    "GitHub\n",
    "3Top/word2vec-api\n",
    "Simple web service providing a word embedding model - 3Top/word2vec-api\n",
    "\n",
    "Tiago   [7 hours ago]\n",
    "thanks!\n",
    "\n",
    "Tinsae   [7 hours ago]\n",
    "The size of google news corpus is 100B. Does that mean 100B words?\n",
    "\n",
    "Tiago   [6 hours ago]\n",
    "I am trying to understand what is that 100B. Because the vocabulary size is 3 million\n",
    "\n",
    "Tiago   [6 hours ago]\n",
    "(and anyway, where did you get the 5 million user reviews? )\n",
    "\n",
    "Tiago   [6 hours ago]\n",
    "(not pointing fingers, just \"oh, cool, I might do some cool stuff with that\")\n",
    "\n",
    "Tinsae   [6 hours ago]\n",
    "Yelp reviews from kaggle\n",
    "\n",
    "Tiago   [6 hours ago]\n",
    "oh, okay\n",
    "\n",
    "Tiago   [6 hours ago]\n",
    "another question: why are you using that word2vec-api?\n",
    "\n",
    "Tinsae   [6 hours ago]\n",
    "I am not using it\n",
    "\n",
    "Tiago   [6 hours ago]\n",
    "oooooooh, okay\n",
    "\n",
    "Tinsae   [6 hours ago]\n",
    "The link is given in the course to check out  pretrained wordnetvectors\n",
    "\n",
    "Tiago   [6 hours ago]\n",
    "FOUND IT: about 100 billion words\n",
    "\n",
    "Tiago   [6 hours ago]\n",
    "https://code.google.com/archive/p/word2vec/\n",
    "\n",
    "Tiago   [6 hours ago]\n",
    "And so, I ask you, how many words do all of your reviews have?\n",
    "\n",
    "Tinsae   [6 hours ago]\n",
    "I don't know the actual number. I created a bag of words with min_df=0.001 and obtained 3000 words out of 1M reviews.\n",
    "\n",
    "Tiago   [6 hours ago]\n",
    "which function did you use for the bag of words?\n",
    "\n",
    "Tinsae   [6 hours ago]\n",
    "CountVectorizer\n",
    "\n",
    "Tiago   [6 hours ago]\n",
    "okay\n",
    "\n",
    "Tiago   [6 hours ago]\n",
    "and, do you have a link to the raw data?\n",
    "\n",
    "Tiago   [6 hours ago]\n",
    "or to the kaggle page\n",
    "\n",
    "Tinsae   [6 hours ago]\n",
    "It will take more than 2 hours to count the words.  I thought we could guess the word count.\n",
    "\n",
    "https://www.kaggle.com/yelp-dataset/yelp-dataset/kernels\n",
    "kaggle.com\n",
    "Yelp Dataset\n",
    "A trove of reviews, businesses, users, tips, and check-in data!\n",
    "\n",
    "Tiago   [6 hours ago]\n",
    "Are you doing the count single-threaded? Only one processor core?\n",
    "\n",
    "Tinsae   [6 hours ago]\n",
    "njobs=-1\n",
    "\n",
    "Tiago   [6 hours ago]\n",
    "where do you use that param njobs?\n",
    "\n",
    "Tinsae   [6 hours ago]\n",
    "I went back to the code to find that I didn't use CountVectorizer in parallelize form. It doesn't have njobs kwarg. (edited)\n",
    "\n",
    "Tinsae   [6 hours ago]\n",
    "But the virtual machine I used  has 8 cores with 30GB ram\n",
    "\n",
    "Tiago   [6 hours ago]\n",
    "another way to check if it uses parallelization under the core is to run the thing and monitor CPU Usage (with something like htop)\n",
    "\n",
    "Tiago   [6 hours ago]\n",
    "(I am saying this, because a looot of times I found something was parallelized by default, got a beefy VM and saw only one processor being used)\n",
    "\n",
    "Tinsae   [6 hours ago]\n",
    "The cpu utilization never crossed 50% (edited)\n",
    "\n",
    "Tiago   [6 hours ago]\n",
    "what did you use to check CPU Usage?\n",
    "\n",
    "Tinsae   [6 hours ago]\n",
    "GCP compute engine has a monitor page for every VM\n",
    "\n",
    "Tiago   [6 hours ago]\n",
    "oh, okay\n",
    "\n",
    "Tiago   [6 hours ago]\n",
    "well, anyway, if it wasn't bigger than 50%, definitely not parallel\n",
    "\n",
    "Tinsae   [6 hours ago]\n",
    "monitoring.png \n",
    "\n",
    "Tiago   [6 hours ago]\n",
    "Well, I see two ways forward here:\n",
    "\n",
    "1) Just train the model and let's see what happens\n",
    "2) I go to google colab and do some parallel code to find out how many words that corpus has\n",
    "\n",
    "Tiago   [6 hours ago]\n",
    "I am wiling to do the second route because not many people here now. But I want to finish my avocado first.\n",
    "\n",
    "Tinsae   [6 hours ago]\n",
    ":avocado: ok.\n",
    "\n",
    "Tinsae   [6 hours ago]\n",
    "Can you download and upload 5GB data easily? It is large json file. (edited)\n",
    "\n",
    "Tiago   [6 hours ago]\n",
    "my idea is to download from kaggle straight to google colab's notebook\n",
    "\n",
    "Tinsae   [6 hours ago]\n",
    "k\n",
    "\n",
    "Tiago   [6 hours ago]\n",
    "btw, if you have any direct links just laying around, now is the time to share\n",
    "\n",
    "Tinsae   [6 hours ago]\n",
    "```# import neccessary libraries\n",
    "from ftplib import FTP\n",
    "import requests\n",
    "\n",
    "# login to ftp server\n",
    "server = \"##.##.###.##\"\n",
    "username = \"******@tinsaealemayehu.com\"\n",
    "password = \"********\"\n",
    "ftp = FTP(server)\n",
    "ftp.login(user=username, passwd=password)\n",
    "\n",
    "rvfile = open(\"yelp_academic_dataset_review.json\", \"wb\")\n",
    "ftp.retrbinary('RETR yelp_academic_dataset_review.json', rvfile.write)\n",
    "rvfile.close()```\n",
    "(edited)\n",
    "\n",
    "Tinsae   [6 hours ago]\n",
    "You can use the above code. I sent you the server, username password through PM (edited)\n",
    "\n",
    "Greg   [6 hours ago]\n",
    "I have trained doc2vec models on considerably less text, and those are just fancy word2vec models, so my 2cents are to go for it.\n",
    "\n",
    "Tinsae   [5 hours ago]\n",
    "Thanks @Greg. Just for fun, I searched \"2cents\" using google news vectors and found a bunch of rock bands\n",
    "'Sworn_Enemy', 0.6559710502624512), ('Hinder_Papa_Roach', 0.6507534980773926), ('KILL_HANNAH', 0.6474124193191528), ('Singer_Jacoby_Shaddix', 0.645458459854126)\n",
    "\n",
    "Tiago   [5 hours ago]\n",
    "Thanks, Greg!\n",
    "\n",
    "Tiago   [5 hours ago]\n",
    "But I think I'll do the useless coding now just for the sake of fun\n",
    "\n",
    "Tiago   [4 hours ago]\n",
    "Aaaand the results are just in: 6685900 words\n",
    "\n",
    "Tiago   [4 hours ago]\n",
    "so, almost 7 million words\n",
    "\n",
    "Tiago   [4 hours ago]\n",
    "in total\n",
    "\n",
    "Tiago   [4 hours ago]\n",
    "now, to count uniques\n",
    "\n",
    "Tinsae   [4 hours ago]\n",
    "Nice!! waiting for the unique counts\n",
    "\n",
    "Tiago   [2 hours ago]\n",
    "Aaand the count just used all the RAM!\n",
    "\n",
    "Tinsae   [2 hours ago]\n",
    "You spent enough time in it.  If there is a chance you could share your code. I would try it in my 56gb ram virtual machine. (edited)\n",
    "\n",
    "Tiago   [2 hours ago]\n",
    "```import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "!wget -c \"https://storage.googleapis.com/kaggle-datasets/10100/277695/yelp-dataset.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1551046405&Signature=p%2BBda9sOLceu8EceqHAhrOPKgraMDDfZ8%2FqiW1z8DygzgoJaUzBW5xL8MWtD5z55OVJWe61Zv9qpdVeHCXcZ0r4mpV7dezhwA2Mmus2OyJqAhhWoXxPSieNB36fHaFmrm6xu%2FOEp5R1TJ2s72vWKU7tnOJS9%2BFBYnvOuV6cPN4lrpVVTrixOjCLv8QQWndfGR7V1Wcz2MqLaqAVFWuN94WU6ycz%2BlcSvdgqPplNHmmzcT%2BBvepxcuTheWMPuHusGUPJ1FHHQ4BEv8RgrhtTvXtV6jxupyw69eMHWpdN0J4bWHACc2%2BGthxgK00Tplt%2FrINAFiPATMf7dpciyNKigAA%3D%3D\" -O yelp.zip\n",
    "!unzip yelp.zip\n",
    "ddf = dd.read_json('yelp_academic_dataset_review.json', blocksize=2**28)\n",
    "textao = ddf.text.to_bag()\n",
    "size_all_itens = textao.count().compute()\n",
    "distinct_items = textao.distinct().compute()```\n",
    "```\n",
    "\n",
    "Tiago   [2 hours ago]\n",
    "soo.... apparently, only 4388 words\n",
    "\n",
    "Tiago   [2 hours ago]\n",
    "that seems too little\n",
    "\n",
    "Tiago   [2 hours ago]\n",
    "aaaand my code is broken\n",
    "\n",
    "Tiago   [2 hours ago]\n",
    "anyway, going to close the ticket because the issue is solved (I guess)\n",
    "\n",
    "Tinsae   [10 minutes ago]\n",
    "Thank you very much. @Tiago. We had a very productive session. I learned a lot of new things (edited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "96px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
