{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Now it's time for another guided example. This time we're going to look at recipes. Specifically we'll use the epicurious dataset, which has a collection of recipes, key terms and ingredients, and their ratings.\n",
    "\n",
    "What we want to see is if we can use the ingredient and keyword list to predict the rating. For someone writing a cookbook this could be really useful information that could help them choose which recipes to include because they're more likely to be enjoyed and therefore make the book more likely to be successful.\n",
    "\n",
    "First let's load the dataset. It's [available on Kaggle](https://www.kaggle.com/hugodarwood/epirecipes). We'll use the csv file here and as pull out column names and some summary statistics for ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('../../../Data & Script/epi_r.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['title',\n",
       " 'rating',\n",
       " 'calories',\n",
       " 'protein',\n",
       " 'fat',\n",
       " 'sodium',\n",
       " '#cakeweek',\n",
       " '#wasteless',\n",
       " '22-minute meals',\n",
       " '3-ingredient recipes',\n",
       " '30 days of groceries',\n",
       " 'advance prep required',\n",
       " 'alabama',\n",
       " 'alaska',\n",
       " 'alcoholic',\n",
       " 'almond',\n",
       " 'amaretto',\n",
       " 'anchovy',\n",
       " 'anise',\n",
       " 'anniversary',\n",
       " 'anthony bourdain',\n",
       " 'aperitif',\n",
       " 'appetizer',\n",
       " 'apple',\n",
       " 'apple juice',\n",
       " 'apricot',\n",
       " 'arizona',\n",
       " 'artichoke',\n",
       " 'arugula',\n",
       " 'asian pear',\n",
       " 'asparagus',\n",
       " 'aspen',\n",
       " 'atlanta',\n",
       " 'australia',\n",
       " 'avocado',\n",
       " 'back to school',\n",
       " 'backyard bbq',\n",
       " 'bacon',\n",
       " 'bake',\n",
       " 'banana',\n",
       " 'barley',\n",
       " 'basil',\n",
       " 'bass',\n",
       " 'bastille day',\n",
       " 'bean',\n",
       " 'beef',\n",
       " 'beef rib',\n",
       " 'beef shank',\n",
       " 'beef tenderloin',\n",
       " 'beer',\n",
       " 'beet',\n",
       " 'bell pepper',\n",
       " 'berry',\n",
       " 'beverly hills',\n",
       " 'birthday',\n",
       " 'biscuit',\n",
       " 'bitters',\n",
       " 'blackberry',\n",
       " 'blender',\n",
       " 'blue cheese',\n",
       " 'blueberry',\n",
       " 'boil',\n",
       " 'bok choy',\n",
       " 'bon appétit',\n",
       " 'bon app��tit',\n",
       " 'boston',\n",
       " 'bourbon',\n",
       " 'braise',\n",
       " 'bran',\n",
       " 'brandy',\n",
       " 'bread',\n",
       " 'breadcrumbs',\n",
       " 'breakfast',\n",
       " 'brie',\n",
       " 'brine',\n",
       " 'brisket',\n",
       " 'broccoli',\n",
       " 'broccoli rabe',\n",
       " 'broil',\n",
       " 'brooklyn',\n",
       " 'brown rice',\n",
       " 'brownie',\n",
       " 'brunch',\n",
       " 'brussel sprout',\n",
       " 'buffalo',\n",
       " 'buffet',\n",
       " 'bulgaria',\n",
       " 'bulgur',\n",
       " 'burrito',\n",
       " 'butter',\n",
       " 'buttermilk',\n",
       " 'butternut squash',\n",
       " 'butterscotch/caramel',\n",
       " 'cabbage',\n",
       " 'cake',\n",
       " 'california',\n",
       " 'calvados',\n",
       " 'cambridge',\n",
       " 'campari',\n",
       " 'camping',\n",
       " 'canada',\n",
       " 'candy',\n",
       " 'candy thermometer',\n",
       " 'cantaloupe',\n",
       " 'capers',\n",
       " 'caraway',\n",
       " 'cardamom',\n",
       " 'carrot',\n",
       " 'cashew',\n",
       " 'casserole/gratin',\n",
       " 'cauliflower',\n",
       " 'caviar',\n",
       " 'celery',\n",
       " 'chambord',\n",
       " 'champagne',\n",
       " 'chard',\n",
       " 'chartreuse',\n",
       " 'cheddar',\n",
       " 'cheese',\n",
       " 'cherry',\n",
       " 'chestnut',\n",
       " 'chicago',\n",
       " 'chicken',\n",
       " 'chickpea',\n",
       " 'chile',\n",
       " 'chile pepper',\n",
       " 'chili',\n",
       " 'chill',\n",
       " 'chive',\n",
       " 'chocolate',\n",
       " 'christmas',\n",
       " 'christmas eve',\n",
       " 'cilantro',\n",
       " 'cinco de mayo',\n",
       " 'cinnamon',\n",
       " 'citrus',\n",
       " 'clam',\n",
       " 'clove',\n",
       " 'cobbler/crumble',\n",
       " 'cocktail',\n",
       " 'cocktail party',\n",
       " 'coconut',\n",
       " 'cod',\n",
       " 'coffee',\n",
       " 'coffee grinder',\n",
       " 'cognac/armagnac',\n",
       " 'collard greens',\n",
       " 'colorado',\n",
       " 'columbus',\n",
       " 'condiment',\n",
       " 'condiment/spread',\n",
       " 'connecticut',\n",
       " 'cook like a diner',\n",
       " 'cookbook critic',\n",
       " 'cookie',\n",
       " 'cookies',\n",
       " 'coriander',\n",
       " 'corn',\n",
       " 'cornmeal',\n",
       " 'costa mesa',\n",
       " 'cottage cheese',\n",
       " 'couscous',\n",
       " 'crab',\n",
       " 'cranberry',\n",
       " 'cranberry sauce',\n",
       " 'cream cheese',\n",
       " 'créme de cacao',\n",
       " 'crêpe',\n",
       " 'cr��me de cacao',\n",
       " 'cuba',\n",
       " 'cucumber',\n",
       " 'cumin',\n",
       " 'cupcake',\n",
       " 'currant',\n",
       " 'curry',\n",
       " 'custard',\n",
       " 'dairy',\n",
       " 'dairy free',\n",
       " 'dallas',\n",
       " 'date',\n",
       " 'deep-fry',\n",
       " 'denver',\n",
       " 'dessert',\n",
       " 'digestif',\n",
       " 'dill',\n",
       " 'dinner',\n",
       " 'dip',\n",
       " 'diwali',\n",
       " 'dominican republic',\n",
       " 'dorie greenspan',\n",
       " 'double boiler',\n",
       " 'dried fruit',\n",
       " 'drink',\n",
       " 'drinks',\n",
       " 'duck',\n",
       " 'easter',\n",
       " 'eau de vie',\n",
       " 'edible gift',\n",
       " 'egg',\n",
       " 'egg nog',\n",
       " 'eggplant',\n",
       " 'egypt',\n",
       " 'emeril lagasse',\n",
       " 'endive',\n",
       " 'engagement party',\n",
       " 'england',\n",
       " 'entertaining',\n",
       " 'epi + ushg',\n",
       " 'epi loves the microwave',\n",
       " 'escarole',\n",
       " 'fall',\n",
       " 'family reunion',\n",
       " 'fat free',\n",
       " \"father's day\",\n",
       " 'fennel',\n",
       " 'feta',\n",
       " 'fig',\n",
       " 'fish',\n",
       " 'flaming hot summer',\n",
       " 'flat bread',\n",
       " 'florida',\n",
       " 'fontina',\n",
       " 'food processor',\n",
       " 'fortified wine',\n",
       " 'fourth of july',\n",
       " 'france',\n",
       " 'frangelico',\n",
       " 'frankenrecipe',\n",
       " 'freeze/chill',\n",
       " 'freezer food',\n",
       " 'friendsgiving',\n",
       " 'frittata',\n",
       " 'fritter',\n",
       " 'frozen dessert',\n",
       " 'fruit',\n",
       " 'fruit juice',\n",
       " 'fry',\n",
       " 'game',\n",
       " 'garlic',\n",
       " 'georgia',\n",
       " 'germany',\n",
       " 'gin',\n",
       " 'ginger',\n",
       " 'goat cheese',\n",
       " 'goose',\n",
       " 'gouda',\n",
       " 'gourmet',\n",
       " 'graduation',\n",
       " 'grains',\n",
       " 'grand marnier',\n",
       " 'granola',\n",
       " 'grape',\n",
       " 'grapefruit',\n",
       " 'grappa',\n",
       " 'green bean',\n",
       " 'green onion/scallion',\n",
       " 'grill',\n",
       " 'grill/barbecue',\n",
       " 'ground beef',\n",
       " 'ground lamb',\n",
       " 'guam',\n",
       " 'guava',\n",
       " 'haiti',\n",
       " 'halibut',\n",
       " 'halloween',\n",
       " 'ham',\n",
       " 'hamburger',\n",
       " 'hanukkah',\n",
       " 'harpercollins',\n",
       " 'hawaii',\n",
       " 'hazelnut',\n",
       " 'healdsburg',\n",
       " 'healthy',\n",
       " 'herb',\n",
       " 'high fiber',\n",
       " 'hollywood',\n",
       " 'hominy/cornmeal/masa',\n",
       " 'honey',\n",
       " 'honeydew',\n",
       " \"hors d'oeuvre\",\n",
       " 'horseradish',\n",
       " 'hot drink',\n",
       " 'hot pepper',\n",
       " 'house & garden',\n",
       " 'house cocktail',\n",
       " 'houston',\n",
       " 'hummus',\n",
       " 'ice cream',\n",
       " 'ice cream machine',\n",
       " 'iced coffee',\n",
       " 'iced tea',\n",
       " 'idaho',\n",
       " 'illinois',\n",
       " 'indiana',\n",
       " 'iowa',\n",
       " 'ireland',\n",
       " 'israel',\n",
       " 'italy',\n",
       " 'jalapeño',\n",
       " 'jam or jelly',\n",
       " 'jamaica',\n",
       " 'japan',\n",
       " 'jerusalem artichoke',\n",
       " 'juicer',\n",
       " 'jícama',\n",
       " 'kahlúa',\n",
       " 'kale',\n",
       " 'kansas',\n",
       " 'kansas city',\n",
       " 'kentucky',\n",
       " 'kentucky derby',\n",
       " 'kid-friendly',\n",
       " 'kidney friendly',\n",
       " 'kirsch',\n",
       " 'kitchen olympics',\n",
       " 'kiwi',\n",
       " 'kosher',\n",
       " 'kosher for passover',\n",
       " 'kumquat',\n",
       " 'kwanzaa',\n",
       " 'labor day',\n",
       " 'lamb',\n",
       " 'lamb chop',\n",
       " 'lamb shank',\n",
       " 'lancaster',\n",
       " 'las vegas',\n",
       " 'lasagna',\n",
       " 'leafy green',\n",
       " 'leek',\n",
       " 'legume',\n",
       " 'lemon',\n",
       " 'lemon juice',\n",
       " 'lemongrass',\n",
       " 'lentil',\n",
       " 'lettuce',\n",
       " 'lima bean',\n",
       " 'lime',\n",
       " 'lime juice',\n",
       " 'lingonberry',\n",
       " 'liqueur',\n",
       " 'lobster',\n",
       " 'london',\n",
       " 'long beach',\n",
       " 'los angeles',\n",
       " 'louisiana',\n",
       " 'louisville',\n",
       " 'low cal',\n",
       " 'low carb',\n",
       " 'low cholesterol',\n",
       " 'low fat',\n",
       " 'low sodium',\n",
       " 'low sugar',\n",
       " 'low/no sugar',\n",
       " 'lunar new year',\n",
       " 'lunch',\n",
       " 'lychee',\n",
       " 'macadamia nut',\n",
       " 'macaroni and cheese',\n",
       " 'maine',\n",
       " 'mandoline',\n",
       " 'mango',\n",
       " 'maple syrup',\n",
       " 'mardi gras',\n",
       " 'margarita',\n",
       " 'marinade',\n",
       " 'marinate',\n",
       " 'marsala',\n",
       " 'marscarpone',\n",
       " 'marshmallow',\n",
       " 'martini',\n",
       " 'maryland',\n",
       " 'massachusetts',\n",
       " 'mayonnaise',\n",
       " 'meat',\n",
       " 'meatball',\n",
       " 'meatloaf',\n",
       " 'melon',\n",
       " 'mexico',\n",
       " 'mezcal',\n",
       " 'miami',\n",
       " 'michigan',\n",
       " 'microwave',\n",
       " 'midori',\n",
       " 'milk/cream',\n",
       " 'minneapolis',\n",
       " 'minnesota',\n",
       " 'mint',\n",
       " 'mississippi',\n",
       " 'missouri',\n",
       " 'mixer',\n",
       " 'molasses',\n",
       " 'monterey jack',\n",
       " 'mortar and pestle',\n",
       " \"mother's day\",\n",
       " 'mozzarella',\n",
       " 'muffin',\n",
       " 'mushroom',\n",
       " 'mussel',\n",
       " 'mustard',\n",
       " 'mustard greens',\n",
       " 'nancy silverton',\n",
       " 'nebraska',\n",
       " 'nectarine',\n",
       " 'new hampshire',\n",
       " 'new jersey',\n",
       " 'new mexico',\n",
       " 'new orleans',\n",
       " \"new year's day\",\n",
       " \"new year's eve\",\n",
       " 'new york',\n",
       " 'no meat, no problem',\n",
       " 'no sugar added',\n",
       " 'no-cook',\n",
       " 'non-alcoholic',\n",
       " 'noodle',\n",
       " 'north carolina',\n",
       " 'nut',\n",
       " 'nutmeg',\n",
       " 'oat',\n",
       " 'oatmeal',\n",
       " 'octopus',\n",
       " 'ohio',\n",
       " 'oklahoma',\n",
       " 'okra',\n",
       " 'oktoberfest',\n",
       " 'olive',\n",
       " 'omelet',\n",
       " 'one-pot meal',\n",
       " 'onion',\n",
       " 'orange',\n",
       " 'orange juice',\n",
       " 'oregano',\n",
       " 'oregon',\n",
       " 'organic',\n",
       " 'orzo',\n",
       " 'oscars',\n",
       " 'oyster',\n",
       " 'pacific palisades',\n",
       " 'paleo',\n",
       " 'pan-fry',\n",
       " 'pancake',\n",
       " 'papaya',\n",
       " 'paprika',\n",
       " 'parade',\n",
       " 'paris',\n",
       " 'parmesan',\n",
       " 'parsley',\n",
       " 'parsnip',\n",
       " 'party',\n",
       " 'pasadena',\n",
       " 'passion fruit',\n",
       " 'passover',\n",
       " 'pasta',\n",
       " 'pasta maker',\n",
       " 'pastry',\n",
       " 'pea',\n",
       " 'peach',\n",
       " 'peanut',\n",
       " 'peanut butter',\n",
       " 'peanut free',\n",
       " 'pear',\n",
       " 'pecan',\n",
       " 'pennsylvania',\n",
       " 'pepper',\n",
       " 'pernod',\n",
       " 'persian new year',\n",
       " 'persimmon',\n",
       " 'peru',\n",
       " 'pescatarian',\n",
       " 'philippines',\n",
       " 'phyllo/puff pastry dough',\n",
       " 'pickles',\n",
       " 'picnic',\n",
       " 'pie',\n",
       " 'pine nut',\n",
       " 'pineapple',\n",
       " 'pistachio',\n",
       " 'pittsburgh',\n",
       " 'pizza',\n",
       " 'plantain',\n",
       " 'plum',\n",
       " 'poach',\n",
       " 'poblano',\n",
       " 'poker/game night',\n",
       " 'pomegranate',\n",
       " 'pomegranate juice',\n",
       " 'poppy',\n",
       " 'pork',\n",
       " 'pork chop',\n",
       " 'pork rib',\n",
       " 'pork tenderloin',\n",
       " 'port',\n",
       " 'portland',\n",
       " 'pot pie',\n",
       " 'potato',\n",
       " 'potato salad',\n",
       " 'potluck',\n",
       " 'poultry',\n",
       " 'poultry sausage',\n",
       " 'pressure cooker',\n",
       " 'prosciutto',\n",
       " 'providence',\n",
       " 'prune',\n",
       " 'pumpkin',\n",
       " 'punch',\n",
       " 'purim',\n",
       " 'quail',\n",
       " 'quiche',\n",
       " 'quick & easy',\n",
       " 'quick and healthy',\n",
       " 'quince',\n",
       " 'quinoa',\n",
       " 'rabbit',\n",
       " 'rack of lamb',\n",
       " 'radicchio',\n",
       " 'radish',\n",
       " 'raisin',\n",
       " 'ramadan',\n",
       " 'ramekin',\n",
       " 'raspberry',\n",
       " 'raw',\n",
       " 'red wine',\n",
       " 'rhode island',\n",
       " 'rhubarb',\n",
       " 'rice',\n",
       " 'ricotta',\n",
       " 'roast',\n",
       " 'root vegetable',\n",
       " 'rosemary',\n",
       " 'rosh hashanah/yom kippur',\n",
       " 'rosé',\n",
       " 'rub',\n",
       " 'rum',\n",
       " 'rutabaga',\n",
       " 'rye',\n",
       " 'saffron',\n",
       " 'sage',\n",
       " 'sake',\n",
       " 'salad',\n",
       " 'salad dressing',\n",
       " 'salmon',\n",
       " 'salsa',\n",
       " 'san francisco',\n",
       " 'sandwich',\n",
       " 'sandwich theory',\n",
       " 'sangria',\n",
       " 'santa monica',\n",
       " 'sardine',\n",
       " 'sauce',\n",
       " 'sausage',\n",
       " 'sauté',\n",
       " 'scallop',\n",
       " 'scotch',\n",
       " 'seafood',\n",
       " 'seattle',\n",
       " 'seed',\n",
       " 'self',\n",
       " 'semolina',\n",
       " 'sesame',\n",
       " 'sesame oil',\n",
       " 'shallot',\n",
       " 'shavuot',\n",
       " 'shellfish',\n",
       " 'sherry',\n",
       " 'shower',\n",
       " 'shrimp',\n",
       " 'side',\n",
       " 'simmer',\n",
       " 'skewer',\n",
       " 'slow cooker',\n",
       " 'smoker',\n",
       " 'smoothie',\n",
       " 'snapper',\n",
       " 'sorbet',\n",
       " 'soufflé/meringue',\n",
       " 'soup/stew',\n",
       " 'sour cream',\n",
       " 'sourdough',\n",
       " 'south carolina',\n",
       " 'soy',\n",
       " 'soy free',\n",
       " 'soy sauce',\n",
       " 'spain',\n",
       " 'sparkling wine',\n",
       " 'spice',\n",
       " 'spinach',\n",
       " 'spirit',\n",
       " 'spring',\n",
       " 'spritzer',\n",
       " 'squash',\n",
       " 'squid',\n",
       " 'st. louis',\n",
       " \"st. patrick's day\",\n",
       " 'steak',\n",
       " 'steam',\n",
       " 'stew',\n",
       " 'stir-fry',\n",
       " 'stock',\n",
       " 'strawberry',\n",
       " 'stuffing/dressing',\n",
       " 'sugar conscious',\n",
       " 'sugar snap pea',\n",
       " 'sukkot',\n",
       " 'summer',\n",
       " 'super bowl',\n",
       " 'suzanne goin',\n",
       " 'sweet potato/yam',\n",
       " 'swiss cheese',\n",
       " 'switzerland',\n",
       " 'swordfish',\n",
       " 'taco',\n",
       " 'tailgating',\n",
       " 'tamarind',\n",
       " 'tangerine',\n",
       " 'tapioca',\n",
       " 'tarragon',\n",
       " 'tart',\n",
       " 'tea',\n",
       " 'tennessee',\n",
       " 'tequila',\n",
       " 'tested & improved',\n",
       " 'texas',\n",
       " 'thanksgiving',\n",
       " 'thyme',\n",
       " 'tilapia',\n",
       " 'tofu',\n",
       " 'tomatillo',\n",
       " 'tomato',\n",
       " 'tortillas',\n",
       " 'tree nut',\n",
       " 'tree nut free',\n",
       " 'triple sec',\n",
       " 'tropical fruit',\n",
       " 'trout',\n",
       " 'tuna',\n",
       " 'turnip',\n",
       " 'utah',\n",
       " \"valentine's day\",\n",
       " 'vanilla',\n",
       " 'veal',\n",
       " 'vegan',\n",
       " 'vegetable',\n",
       " 'vegetarian',\n",
       " 'venison',\n",
       " 'vermont',\n",
       " 'vermouth',\n",
       " 'vinegar',\n",
       " 'virginia',\n",
       " 'vodka',\n",
       " 'waffle',\n",
       " 'walnut',\n",
       " 'wasabi',\n",
       " 'washington',\n",
       " 'washington, d.c.',\n",
       " 'watercress',\n",
       " 'watermelon',\n",
       " 'wedding',\n",
       " 'weelicious',\n",
       " 'west virginia',\n",
       " 'westwood',\n",
       " 'wheat/gluten-free',\n",
       " 'whiskey',\n",
       " 'white wine',\n",
       " 'whole wheat',\n",
       " 'wild rice',\n",
       " 'windsor',\n",
       " 'wine',\n",
       " 'winter',\n",
       " 'wisconsin',\n",
       " 'wok',\n",
       " 'yellow squash',\n",
       " 'yogurt',\n",
       " 'yonkers',\n",
       " 'yuca',\n",
       " 'zucchini',\n",
       " 'cookbooks',\n",
       " 'leftovers',\n",
       " 'snack',\n",
       " 'snack week',\n",
       " 'turkey']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(raw_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    20052.000000\n",
       "mean         3.714467\n",
       "std          1.340829\n",
       "min          0.000000\n",
       "25%          3.750000\n",
       "50%          4.375000\n",
       "75%          4.375000\n",
       "max          5.000000\n",
       "Name: rating, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.rating.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "We learn a few things from this analysis. From a ratings perspective, there are just over 20,000 recipes with an average rating of 3.71. What is interesting is that the 25th percentile is actually above the mean. This means there is likely some kind of outlier population. This makes sense when we think about reviews: some bad recipes may have very few very low reviews.\n",
    "\n",
    "Let's validate the idea a bit further with a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "raw_data.rating.hist(bins=20)\n",
    "plt.title('Histogram of Recipe Ratings')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "So a few things are shown in this histogram. Firstly there are sharp discontinutities. We don't have continuous data. No recipe has a 3.5 rating, for example. Also we see the anticipated increase at 0.\n",
    "\n",
    "Let's try a naive approach again, this time using SVM Regressor. But first, we'll have to do a bit of data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Count nulls \n",
    "null_count = raw_data.isnull().sum()\n",
    "null_count[null_count > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "What we can see right away is that nutrition information is not available for all goods. Now this would be an interesting data point, but let's focus on ingredients and keywords right now. So we'll actually drop the whole columns for calories, protein, fat, and sodium. We'll come back to nutrition information later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# # takes long time\n",
    "# from sklearn.svm import SVR\n",
    "# svr = SVR()\n",
    "X = raw_data.drop(['rating', 'title', 'calories', 'protein', 'fat', 'sodium'], 1)\n",
    "y = raw_data.rating\n",
    "# svr.fit(X,Y)\n",
    "\n",
    "# load saved model\n",
    "svr = pickle.load(open('svr1.sav', 'rb'))\n",
    "svr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "__Note that this actually takes quite a while to run, compared to some of the models we've done before. Be patient.__ It's because of the number of features we have.\n",
    "\n",
    "Let's see what a scatter plot looks like, comparing actuals to predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(y, svr.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Now that is a pretty useless visualization. This is because of the discontinous nature of our outcome variable. There's too much data for us to really see what's going on here. If you wanted to look at it you could create histograms, here we'll move on to the scores of both our full fit model and with cross validation. Again if you choose to run it again it will take some time, so you probably shouldn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "svr.score(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(svr, X, Y, n_jobs=-1, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Oh dear, so this did seem not to work very well. In fact it is remarkably poor. Now there are many things that we could do here. \n",
    "\n",
    "Firstly the overfit is a problem, even though it was poor in the first place. We could go back and clean up our feature set. There might be some gains to be made by getting rid of the noise.\n",
    "\n",
    "We could also see how removing the nulls but including dietary information performs. Though its a slight change to the question we could still possibly get some improvements there.\n",
    "\n",
    "Lastly, we could take our regression problem and turn it into a classifier. With this number of features and a discontinuous outcome, we might have better luck thinking of this as a classification problem. We could make it simpler still by instead of classifying on each possible value, group reviews to some decided high and low values.\n",
    "\n",
    "__And that is your challenge.__\n",
    "\n",
    "Transform this regression problem into a binary classifier and clean up the feature set. You can choose whether or not to include nutritional information, but try to cut your feature set down to the 30 most valuable features.\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "When you've finished that, also take a moment to think about bias. Is there anything in this dataset that makes you think it could be biased, perhaps extremely so?\n",
    "\n",
    "There is. Several things in fact, but most glaringly is that we don't actually have a random sample. It could be, and probably is, that the people more likely to choose some kinds of recipes are more likely to give high reviews.\n",
    "\n",
    "After all, people who eat chocolate _might_ just be happier people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Recipe: title, rating (2 features)\n",
    "* ingredients: calories, protein, fat, sodium (4 features)\n",
    "* key_terms: (674 features)\n",
    "\n",
    "Only key_terms are used as predictors. Rating is the target variable\n",
    "\n",
    "Read this \n",
    "http://pbpython.com/categorical-encoding.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Classification\n",
    "#### Value Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.rating.value_counts().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = raw_data.drop(['rating', 'title', 'calories', 'protein', 'fat', 'sodium'], 1)\n",
    "y = raw_data.rating\n",
    "# convert y to categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "lab_enc = LabelEncoder()\n",
    "y = lab_enc.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after encoding\n",
    "pd.Series(y).value_counts().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoding somewhat arbitrary, It is ok for now. I can manually map them when needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recursive Feature Elimination using Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Features using Random Forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFECV\n",
    "# The \"accuracy\" scoring is proportional to the number of correct classifications\n",
    "# takes 30 minutes, better to load previously saved model\n",
    "selector_rf = pickle.load(open('selector_rf.sav', 'rb'))\n",
    "#selector_rf = RFECV(RandomForestClassifier(), scoring='accuracy', n_jobs=-1)\n",
    "#import time\n",
    "#start_time = time.time()\n",
    "#selector_rf.fit(X, y)\n",
    "print(\"Optimal number of features : %d\" % selector_rf.n_features_)\n",
    "print(X.columns[selector_rf.get_support(indices=True)])\n",
    "print(selector_rf.score(X,y))\n",
    "#print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "plt.plot(range(1, len(selector_rf.grid_scores_) + 1), selector_rf.grid_scores_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying to rank the features, but not working because the estimetor is not fitted using fit()\n",
    "#selector_rf.estimator.feature_importances_\n",
    "\n",
    "selector_rf.grid_scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "sklearn_pca = PCA(n_components=30)\n",
    "X_t = sklearn_pca.fit_transform(X)\n",
    "\n",
    "print(\n",
    "    'The percentage of total variance in the dataset explained by each',\n",
    "    'component from Sklearn PCA.\\n',\n",
    "    sklearn_pca.explained_variance_ratio_\n",
    ")\n",
    "# take the first principal component and find the loading scores(163 of them)\n",
    "loading_scores = pd.Series(sklearn_pca.components_[0], index=X.columns)\n",
    "\n",
    "sorted_loading_scores = loading_scores.abs().sort_values(ascending=False)\n",
    "\n",
    "sorted_loading_scores.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the cofficents don't add up to 1, but the magnitude of an eigen vector is 1.0\n",
    "print(np.linalg.norm(sklearn_pca.components_[0]))\n",
    "print(np.linalg.norm(sklearn_pca.components_[1]))\n",
    "print(np.linalg.norm(sklearn_pca.components_[2]))\n",
    "print(np.linalg.norm(sklearn_pca.components_[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "# train SVC model(takes time)\n",
    "from sklearn.svm import SVC\n",
    "#modelA =  SVC(tol=0.01, max_iter=-1)\n",
    "# Use PCA features\n",
    "#modelA.fit(X_t,y)\n",
    "#print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "modelA = pickle.load(open('modelA.sav', 'rb'))\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(modelA, X_t, y, n_jobs=-1, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use original features\n",
    "X = raw_data[sorted_loading_scores.index]\n",
    "import time\n",
    "start_time = time.time()\n",
    "# train SVC model(takes time)\n",
    "from sklearn.svm import SVC\n",
    "#modelB =  SVC(tol=0.01, max_iter=-1)\n",
    "#modelB.fit(X,y)\n",
    "#print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "modelB = pickle.load(open(\"modelB.sav\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(modelB, X, y, n_jobs=-1, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "# zero or one in 80% of the samples\n",
    "# one class cannot be more than 91% of the data\n",
    "var_selector = VarianceThreshold(.9115 * (1 - .9115))\n",
    "X_t2 = var_selector.fit_transform(X)\n",
    "print(X.columns[var_selector.get_support(indices=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare it with PCA\n",
    "pca_and_variance_th = pd.DataFrame({\"PCA_Importance\": sorted_loading_scores.head(30).index, \"Variance_Threshold\":X.columns[var_selector.get_support(indices=True)]})\n",
    "print(pca_and_variance_th)\n",
    "\n",
    "# how much intersection\n",
    "print(len(set(set(pca_and_variance_th.PCA_Importance).intersection(set(pca_and_variance_th.Variance_Threshold)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "# train SVC model(takes time)\n",
    "from sklearn.svm import SVC\n",
    "#modelC =  SVC(tol=0.01, max_iter=-1)\n",
    "#modelC.fit(X_t2,y)\n",
    "modelC = pickle.load(open('modelC.sav', 'rb'))\n",
    "#print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelC.score(X_t2,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# from sklearn.feature_selection import RFECV \n",
    "# selector_three = RFECV(estimator= SVC(tol=0.01), step=1, cv=StratifiedKFold(10),\n",
    "#               n_jobs=-1, scoring='accuracy')\n",
    "# selector_three.fit(X, y)\n",
    "# print(X.columns[selector_three.get_support(indices=True)])\n",
    "\n",
    "# print(\"Optimal number of features : %d\" % selector_three.n_features_)\n",
    "# print(X.columns[selector_three.get_support(indices=True)])\n",
    "# print(\"score: \", selector_three.score(X, y))\n",
    "# # Plot number of features VS. cross-validation scores\n",
    "# plt.figure()\n",
    "# plt.xlabel(\"Number of features selected\")\n",
    "# plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "# plt.plot(range(1, len(selector_three.grid_scores_) + 1), selector_three.grid_scores_)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Classfier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = raw_data.rating\n",
    "# split at the median\n",
    "print(y.median())\n",
    "print(\"Nice class balance: \\n\", np.bincount(np.where(y < 4.375, 0, 1)))\n",
    "y_binary = np.where(y < 4.375, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"majority classifier output: \", 10738 / (10738 + 9314))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## try PCA important features\n",
    "# use original features\n",
    "X = raw_data[sorted_loading_scores.index]\n",
    "#import time\n",
    "#start_time = time.time()\n",
    "# train SVC model(takes time)\n",
    "#from sklearn.svm import SVC\n",
    "#modelD =  SVC(tol=0.01, max_iter=-1)\n",
    "#modelD.fit(X, y_binary)\n",
    "modelD = pickle.load(open('modelD.sav', 'rb'))\n",
    "#print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(modelD, X, y_binary, n_jobs=-1, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## try PCA components\n",
    "\n",
    "#import time\n",
    "#start_time = time.time()\n",
    "# train SVC model(takes time)\n",
    "from sklearn.svm import SVC\n",
    "modelE = pickle.load(open('modelE.sav', 'rb'))\n",
    "\n",
    "#modelE =  SVC(tol=0.01, max_iter=-1)\n",
    "#modelE.fit(X_t, y_binary)\n",
    "#print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(modelE, X_t, y_binary, n_jobs=-1, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Features using Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFECV\n",
    "# The \"accuracy\" scoring is proportional to the number of correct classifications\n",
    "# takes 30 minutes, better to load previously saved model\n",
    "selector_rf2 = pickle.load(open('selector_rf2.sav', 'rb'))\n",
    "#selector_rf2 = RFECV(RandomForestClassifier(), scoring='accuracy', n_jobs=-1)\n",
    "#import time\n",
    "#start_time = time.time()\n",
    "#selector_rf2.fit(X, y_binary)\n",
    "print(\"Optimal number of features : %d\" % selector_rf2.n_features_)\n",
    "print(X.columns[selector_rf2.get_support(indices=True)])\n",
    "print(selector_rf2.score(X, y_binary))\n",
    "#print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "plt.plot(range(1, len(selector_rf2.grid_scores_) + 1), selector_rf2.grid_scores_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recursive Feature Elimination(with Max features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "#start_time = time.time()\n",
    "from sklearn.feature_selection import RFE\n",
    "# takes 10 minutes\n",
    "#selector_rf3 = RFE(RandomForestClassifier(), n_features_to_select=30, step=1)\n",
    "#selector_rf3 = selector_rf3.fit(X, y_binary)\n",
    "selector_rf3 = pickle.load(open(\"selector_rf3.sav\",\"rb\"))\n",
    "#print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.columns[selector_rf3.get_support(indices=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare with the other feature selection results\n",
    "\n",
    "allthree = pd.DataFrame({\"PCA_Importance\": sorted_loading_scores.head(30).index, \n",
    "                         \"Variance_Threshold\":X.columns[var_selector.get_support(indices=True)],\n",
    "                         \"Random Forest\": X.columns[selector_rf3.get_support(indices=True)]\n",
    "                        })\n",
    "print(allthree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform X\n",
    "Xt_rf = selector_rf3.transform(X)\n",
    "Xt_rf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "#modelF =  SVC(tol=0.01, max_iter=-1)\n",
    "#modelF.fit(X_t2,y_binary)\n",
    "modelF = pickle.load(open('modelF.sav', 'rb'))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(modelF, Xt_rf, y_binary, n_jobs=-1, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the models\n",
    "import pickle\n",
    "pickle.dump(svr, open('svr1.sav', 'wb'))\n",
    "pickle.dump(selector_rf, open('selector_rf.sav', 'wb'))\n",
    "pickle.dump(modelA, open('modelA.sav', 'wb'))\n",
    "pickle.dump(modelB, open('modelB.sav', 'wb'))\n",
    "pickle.dump(modelC, open('modelC.sav', 'wb'))\n",
    "pickle.dump(selector_rf2, open('selector_rf2.sav', 'wb'))\n",
    "pickle.dump(modelD, open('modelD.sav', 'wb'))\n",
    "pickle.dump(modelE, open('modelE.sav', 'wb'))\n",
    "pickle.dump(selector_rf3, open('selector_rf3.sav', 'wb'))\n",
    "pickle.dump(modelF, open('modelF.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reflection\n",
    "\n",
    "Support Vector Machines can be used for Regression and Classification. For Classification we want to maximize the margin which is the distance from the classifier hyperplane(line for 2D data) to the support vectors(nereast vector to the boundary). If there are misclassified data points we want to minimize their distance to the boundary. For Regression, the margin is the difference between actual and predicted values. The advantage of SVR over other techniques like OLS is, we can specify the size of the margin and the penality for being outside the margin for training. \n",
    "\n",
    "There are many ways to select features. I run PCA and selected 30 features in two ways: the principal components(modelA), important features(modelB). Then I selected 30 features using VarianceThreshold(modelC). All performed similarly(around 0.4)\n",
    "In this notebook, I tried to do multi-class SVC. \n",
    "\n",
    "For binary target variable, using the PCA fetures, the accuracy score is around 0.575. It is a little higher than the majority class(\n",
    "I experimeted RFECV with randomforest for multiclass (rf1) and for binary ouput(rf2). Both yielded large number of features(>500). I couldn't not find a way to rank them. \n",
    "\n",
    "I found using RFE it is possible to get feature_importances. RFE with SVC is not possible because SVC don't have feature_importances or coef_ attributes. I used RFE with RandomForest and its accuracy is only 0.53."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
