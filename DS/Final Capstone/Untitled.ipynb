{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 # you need to install opencv library, coz it will help in \n",
    "           # getting the peaks in the image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell 2\n",
    "\n",
    "# Each image has a mid line from where the peaks start.\n",
    "# we are trying to detect the row number of that line in the image.\n",
    "# The way we are detecting it is through the colour pixels in the image.\n",
    "# Each blue colour pixel in the image has the RGB values (0,114,189)\n",
    "# so if a row contains more than 50 pixels consecutively which contains this\n",
    "# colour, we conclude that it is the middle line.\n",
    "\n",
    "def get_mid_line(image):\n",
    "    count = 0\n",
    "    for y in range(len(image)):\n",
    "        for x in range(len(image[y])):\n",
    "            if (np.array_equal(image[y][x], np.array([0,114,189]))):\n",
    "                count += 1\n",
    "            else:\n",
    "                count = 0\n",
    "            if (count >= 50):\n",
    "                return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell 3\n",
    "\n",
    "# now we will get the sum of the peaks in the given image\n",
    "# To do this we use opencv's cornerHarris function.\n",
    "# this function gives the corners in an image which in our case is peaks.\n",
    "# we get the coordinates of these peaks through the cornerHarris function, \n",
    "# and we subtract it from the coordinates of mid line in order to find the\n",
    "# height of the peak. Then we add all these heights to get the sum of the \n",
    "# heights which is our feature for classifying the image.\n",
    "\n",
    "def give_peak_sum(file):\n",
    "    image = cv2.imread(file) # opencv's image read function\n",
    "    image_copy = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # converts image from\n",
    "    # BGR color space to RGB color space\n",
    "    \n",
    "    # converting to gray scale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    gray = np.float32(gray)\n",
    "    \n",
    "    # detect corners\n",
    "    dst = cv2.cornerHarris(gray, 2, 3, 0.04)\n",
    "    \n",
    "    # dilate corner image to enhance corner points\n",
    "    dst = cv2.dilate(dst, None)\n",
    "    \n",
    "    thresh = 0.02*dst.max()\n",
    "    \n",
    "    peak_sum = 0\n",
    "    mid_line = get_mid_line(image_copy) # using the previously defined function\n",
    "    \n",
    "    for j in range(0, dst.shape[0]):\n",
    "        for i in range(0, dst.shape[1]):\n",
    "            if (dst[j, i] > thresh):\n",
    "                peak_sum += abs(j-mid_line)\n",
    "                \n",
    "    return (peak_sum)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134 94\n"
     ]
    }
   ],
   "source": [
    "# cell 4\n",
    "\n",
    "# Here we are just collecting the data that we have\n",
    "\n",
    "full_images = []\n",
    "bottom_images = []\n",
    "\n",
    "for file in os.listdir(\"full\"): # I don't know the path of the data files\n",
    "                                # in your system, modify the path accordingly.\n",
    "    full_images.append(file)\n",
    "    \n",
    "for file in os.listdir(\"bottom\"):\n",
    "    bottom_images.append(file)\n",
    "    \n",
    "print (len(full_images), len(bottom_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134 94\n"
     ]
    }
   ],
   "source": [
    "#cell 5\n",
    "\n",
    "# Here for each image, we will get the sum of peaks. This is the main\n",
    "# training step, it might take around 30-45 minutes to run depending on your\n",
    "# system capabilities\n",
    "\n",
    "data_full = []\n",
    "data_bottom = []\n",
    "\n",
    "for file in full_images:\n",
    "    data_full.append((give_peak_sum(\"full/\"+file),0))\n",
    "    \n",
    "for file in bottom_images:\n",
    "    data_bottom.append((give_peak_sum(\"bottom/\"+file), 1))\n",
    "    \n",
    "print (len(data_full), len(data_bottom))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-f6fb1a6eece1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_full\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdata_bottom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'random' is not defined"
     ]
    }
   ],
   "source": [
    "# cell 6\n",
    "\n",
    "# Here we mix the two lists data_full and data_bottom and shuffle it for \n",
    "# randomness\n",
    "\n",
    "data = data_full + data_bottom\n",
    "random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell 7\n",
    "\n",
    "# here we take features in x variable and labels in y variable because\n",
    "# skikit learn libraries require it differently\n",
    "\n",
    "x = [[each[0]] for each in data]\n",
    "y = [[each[1]] for each in data]\n",
    "print (len(x), len(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 8\n",
    "\n",
    "# Here we split our dataset of total 228 images into training and testing\n",
    "# datasets\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "print (len(x_train), len(x_test))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell 9\n",
    "\n",
    "# Here we train our data on a Random Forest Classifier algorithm\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=0)\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 10\n",
    "\n",
    "# Here we test the accuracy of our model\n",
    "\n",
    "correct = 0\n",
    "for i in range(len(x_test)):\n",
    "    if (clf.predict([x_test[i]]) == y_test[i]):\n",
    "        correct += 1\n",
    "\n",
    "print (correct/float(len(x_test))) # this will print out the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# #cell 2\n",
    "\n",
    "# # Each image has a mid line from where the peaks start.\n",
    "# # we are trying to detect the row number of that line in the image.\n",
    "# # The way we are detecting it is through the colour pixels in the image.\n",
    "# # Each blue colour pixel in the image has the RGB values (0,114,189)\n",
    "# # so if a row contains more than 50 pixels consecutively which contains this\n",
    "# # colour, we conclude that it is the middle line.\n",
    "\n",
    "# def get_mid_line(image):\n",
    "#     count = 0\n",
    "#     for y in range(len(image)):\n",
    "#         for x in range(len(image[y])):\n",
    "#             if (np.array_equal(image[y][x], np.array([0,114,189]))):\n",
    "#                 count += 1\n",
    "#             else:\n",
    "#                 count = 0\n",
    "#             if (count >= 50):\n",
    "#                 return y\n",
    "\n",
    "\n",
    "# #cell 3\n",
    "\n",
    "# # now we will get the sum of the peaks in the given image\n",
    "# # To do this we use opencv's cornerHarris function.\n",
    "# # this function gives the corners in an image which in our case is peaks.\n",
    "# # we get the coordinates of these peaks through the cornerHarris function, \n",
    "# # and we subtract it from the coordinates of mid line in order to find the\n",
    "# # height of the peak. Then we add all these heights to get the sum of the \n",
    "# # heights which is our feature for classifying the image.\n",
    "\n",
    "# def give_peak_sum(file):\n",
    "#     image = cv2.imread(file) # opencv's image read function\n",
    "#     image_copy = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # converts image from\n",
    "#     # BGR color space to RGB color space\n",
    "    \n",
    "#     # converting to gray scale\n",
    "#     gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "#     gray = np.float32(gray)\n",
    "    \n",
    "#     # detect corners\n",
    "#     dst = cv2.cornerHarris(gray, 2, 3, 0.04)\n",
    "    \n",
    "#     # dilate corner image to enhance corner points\n",
    "#     dst = cv2.dilate(dst, None)\n",
    "    \n",
    "#     thresh = 0.02*dst.max()\n",
    "    \n",
    "#     peak_sum = 0\n",
    "#     mid_line = get_mid_line(image_copy) # using the previously defined function\n",
    "    \n",
    "#     for j in range(0, dst.shape[0]):\n",
    "#         for i in range(0, dst.shape[1]):\n",
    "#             if (dst[j, i] > thresh):\n",
    "#                 peak_sum += abs(j-mid_line)\n",
    "                \n",
    "#     return (peak_sum)\n",
    "    \n",
    "    \n",
    "# # cell 4\n",
    "\n",
    "# # Here we are just collecting the data that we have\n",
    "\n",
    "# full_images = []\n",
    "# bottom_images = []\n",
    "\n",
    "# for file in os.listdir(\"full\"): # I don't know the path of the data files\n",
    "#                                 # in your system, modify the path accordingly.\n",
    "#     full_images.append(file)\n",
    "    \n",
    "# for file in os.listdir(\"bottom\")\n",
    "#     bottom_images.append(file)\n",
    "    \n",
    "# print (len(full_images), len(bottom_images))\n",
    "\n",
    "\n",
    "# #cell 5\n",
    "\n",
    "# # Here for each image, we will get the sum of peaks. This is the main\n",
    "# # training step, it might take around 30-45 minutes to run depending on your\n",
    "# # system capabilities\n",
    "\n",
    "# data_full = []\n",
    "# data_bottom = []\n",
    "\n",
    "# for file in full_images:\n",
    "#     data_full.append((give_peak_sum(\"full/\"+file),0))\n",
    "    \n",
    "# for file in bottom_images:\n",
    "#     data_bottom.append((give_peak_sum(\"bottom/\"+file), 1))\n",
    "    \n",
    "# print (len(data_full), len(data_bottom))\n",
    "\n",
    "\n",
    "# # cell 6\n",
    "\n",
    "# # Here we mix the two lists data_full and data_bottom and shuffle it for \n",
    "# # randomness\n",
    "\n",
    "# data = data_full + data_bottom\n",
    "# random.shuffle(data)\n",
    "\n",
    "# #cell 7\n",
    "\n",
    "# # here we take features in x variable and labels in y variable because\n",
    "# # skikit learn libraries require it differently\n",
    "\n",
    "# x = [[each[0]] for each in data]\n",
    "# y = [[each[1]] for each in data]\n",
    "# print (len(x), len(y))\n",
    "\n",
    "\n",
    "# # cell 8\n",
    "\n",
    "# # Here we split our dataset of total 228 images into training and testing\n",
    "# # datasets\n",
    "\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "# print (len(x_train), len(x_test))\n",
    "\n",
    "\n",
    "# #cell 9\n",
    "\n",
    "# # Here we train our data on a Random Forest Classifier algorithm\n",
    "\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# clf = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=0)\n",
    "# clf.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# # cell 10\n",
    "\n",
    "# # Here we test the accuracy of our model\n",
    "\n",
    "# correct = 0\n",
    "# for i in range(len(x_test)):\n",
    "#     if (clf.predict([x_test[i]]) == y_test[i]):\n",
    "#         correct += 1\n",
    "\n",
    "# print (correct/float(len(x_test))) # this will print out the accuracy of the model.\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
